{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loaded glmnet 2.0-12\n",
      "\n",
      "\n",
      "Attaching package: ‘dplyr’\n",
      "\n",
      "The following object is masked from ‘package:xgboost’:\n",
      "\n",
      "    slice\n",
      "\n",
      "The following objects are masked from ‘package:stats’:\n",
      "\n",
      "    filter, lag\n",
      "\n",
      "The following objects are masked from ‘package:base’:\n",
      "\n",
      "    intersect, setdiff, setequal, union\n",
      "\n",
      "\n",
      "Attaching package: ‘tidyr’\n",
      "\n",
      "The following object is masked from ‘package:Matrix’:\n",
      "\n",
      "    expand\n",
      "\n",
      "Type 'citation(\"pROC\")' for a citation.\n",
      "\n",
      "Attaching package: ‘pROC’\n",
      "\n",
      "The following object is masked from ‘package:glmnet’:\n",
      "\n",
      "    auc\n",
      "\n",
      "The following objects are masked from ‘package:stats’:\n",
      "\n",
      "    cov, smooth, var\n",
      "\n",
      "\n",
      "Attaching package: ‘gplots’\n",
      "\n",
      "The following object is masked from ‘package:stats’:\n",
      "\n",
      "    lowess\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>'fst'</li>\n",
       "\t<li>'caTools'</li>\n",
       "\t<li>'caret'</li>\n",
       "\t<li>'lattice'</li>\n",
       "\t<li>'stringr'</li>\n",
       "\t<li>'ROCR'</li>\n",
       "\t<li>'gplots'</li>\n",
       "\t<li>'pROC'</li>\n",
       "\t<li>'tidyr'</li>\n",
       "\t<li>'dplyr'</li>\n",
       "\t<li>'glmnet'</li>\n",
       "\t<li>'foreach'</li>\n",
       "\t<li>'Matrix'</li>\n",
       "\t<li>'xgboost'</li>\n",
       "\t<li>'ggplot2'</li>\n",
       "\t<li>'RColorBrewer'</li>\n",
       "\t<li>'stats'</li>\n",
       "\t<li>'graphics'</li>\n",
       "\t<li>'grDevices'</li>\n",
       "\t<li>'utils'</li>\n",
       "\t<li>'datasets'</li>\n",
       "\t<li>'methods'</li>\n",
       "\t<li>'base'</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 'fst'\n",
       "\\item 'caTools'\n",
       "\\item 'caret'\n",
       "\\item 'lattice'\n",
       "\\item 'stringr'\n",
       "\\item 'ROCR'\n",
       "\\item 'gplots'\n",
       "\\item 'pROC'\n",
       "\\item 'tidyr'\n",
       "\\item 'dplyr'\n",
       "\\item 'glmnet'\n",
       "\\item 'foreach'\n",
       "\\item 'Matrix'\n",
       "\\item 'xgboost'\n",
       "\\item 'ggplot2'\n",
       "\\item 'RColorBrewer'\n",
       "\\item 'stats'\n",
       "\\item 'graphics'\n",
       "\\item 'grDevices'\n",
       "\\item 'utils'\n",
       "\\item 'datasets'\n",
       "\\item 'methods'\n",
       "\\item 'base'\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 'fst'\n",
       "2. 'caTools'\n",
       "3. 'caret'\n",
       "4. 'lattice'\n",
       "5. 'stringr'\n",
       "6. 'ROCR'\n",
       "7. 'gplots'\n",
       "8. 'pROC'\n",
       "9. 'tidyr'\n",
       "10. 'dplyr'\n",
       "11. 'glmnet'\n",
       "12. 'foreach'\n",
       "13. 'Matrix'\n",
       "14. 'xgboost'\n",
       "15. 'ggplot2'\n",
       "16. 'RColorBrewer'\n",
       "17. 'stats'\n",
       "18. 'graphics'\n",
       "19. 'grDevices'\n",
       "20. 'utils'\n",
       "21. 'datasets'\n",
       "22. 'methods'\n",
       "23. 'base'\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       " [1] \"fst\"          \"caTools\"      \"caret\"        \"lattice\"      \"stringr\"     \n",
       " [6] \"ROCR\"         \"gplots\"       \"pROC\"         \"tidyr\"        \"dplyr\"       \n",
       "[11] \"glmnet\"       \"foreach\"      \"Matrix\"       \"xgboost\"      \"ggplot2\"     \n",
       "[16] \"RColorBrewer\" \"stats\"        \"graphics\"     \"grDevices\"    \"utils\"       \n",
       "[21] \"datasets\"     \"methods\"      \"base\"        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading required package: DBI\n",
      "Loading required package: stats4\n",
      "Loading required package: BiocGenerics\n",
      "Loading required package: parallel\n",
      "\n",
      "Attaching package: ‘BiocGenerics’\n",
      "\n",
      "The following objects are masked from ‘package:parallel’:\n",
      "\n",
      "    clusterApply, clusterApplyLB, clusterCall, clusterEvalQ,\n",
      "    clusterExport, clusterMap, parApply, parCapply, parLapply,\n",
      "    parLapplyLB, parRapply, parSapply, parSapplyLB\n",
      "\n",
      "The following object is masked from ‘package:pROC’:\n",
      "\n",
      "    var\n",
      "\n",
      "The following objects are masked from ‘package:dplyr’:\n",
      "\n",
      "    combine, intersect, setdiff, union\n",
      "\n",
      "The following objects are masked from ‘package:Matrix’:\n",
      "\n",
      "    colMeans, colSums, rowMeans, rowSums, which\n",
      "\n",
      "The following objects are masked from ‘package:stats’:\n",
      "\n",
      "    IQR, mad, sd, var, xtabs\n",
      "\n",
      "The following objects are masked from ‘package:base’:\n",
      "\n",
      "    anyDuplicated, append, as.data.frame, cbind, colMeans, colnames,\n",
      "    colSums, do.call, duplicated, eval, evalq, Filter, Find, get, grep,\n",
      "    grepl, intersect, is.unsorted, lapply, lengths, Map, mapply, match,\n",
      "    mget, order, paste, pmax, pmax.int, pmin, pmin.int, Position, rank,\n",
      "    rbind, Reduce, rowMeans, rownames, rowSums, sapply, setdiff, sort,\n",
      "    table, tapply, union, unique, unsplit, which, which.max, which.min\n",
      "\n",
      "Loading required package: S4Vectors\n",
      "\n",
      "Attaching package: ‘S4Vectors’\n",
      "\n",
      "The following object is masked from ‘package:caTools’:\n",
      "\n",
      "    runmean\n",
      "\n",
      "The following object is masked from ‘package:gplots’:\n",
      "\n",
      "    space\n",
      "\n",
      "The following object is masked from ‘package:tidyr’:\n",
      "\n",
      "    expand\n",
      "\n",
      "The following objects are masked from ‘package:dplyr’:\n",
      "\n",
      "    first, rename\n",
      "\n",
      "The following object is masked from ‘package:Matrix’:\n",
      "\n",
      "    expand\n",
      "\n",
      "The following object is masked from ‘package:base’:\n",
      "\n",
      "    expand.grid\n",
      "\n",
      "Loading required package: IRanges\n",
      "\n",
      "Attaching package: ‘IRanges’\n",
      "\n",
      "The following objects are masked from ‘package:dplyr’:\n",
      "\n",
      "    collapse, desc, slice\n",
      "\n",
      "The following object is masked from ‘package:xgboost’:\n",
      "\n",
      "    slice\n",
      "\n",
      "Loading required package: GenomeInfoDb\n"
     ]
    }
   ],
   "source": [
    "libs <- c(\n",
    "    'RColorBrewer',\n",
    "    'ggplot2',\n",
    "    'xgboost',\n",
    "    'glmnet',\n",
    "    'dplyr',\n",
    "    'tidyr',\n",
    "    'pROC',\n",
    "    'ROCR',\n",
    "    'stringr',\n",
    "    'caret',\n",
    "    'caTools',\n",
    "    'fst'\n",
    ")\n",
    "\n",
    "for (lib in libs) {\n",
    "        if (!require(lib, character.only = TRUE, quietly = TRUE)) {\n",
    "            install.packages(lib, repos='http://cran.us.r-project.org')\n",
    "        }\n",
    "}\n",
    "\n",
    "(.packages())\n",
    "\n",
    "source(\"my_R_functions/utility_functions.R\")\n",
    "source(\"my_R_functions/stat_functions.R\")\n",
    "source(\"my_R_functions/plot_functions.R\")\n",
    "source(\"/ssd/mrichard/github/BDDS/trenadb/src/utils.R\")\n",
    "source(\"/ssd/mrichard/github/BDDS/footprints/testdb/src/dbFunctions.R\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>motif</th><th scope=col>TF</th><th scope=col>class</th><th scope=col>family</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>MA0001.1                             </td><td>AGL3                                 </td><td>Other Alpha-Helix                    </td><td>MADS                                 </td></tr>\n",
       "\t<tr><td>MA0002.1                             </td><td>RUNX1                                </td><td>Ig-fold                              </td><td>Runt                                 </td></tr>\n",
       "\t<tr><td>MA0003.1                             </td><td>TFAP2A                               </td><td>Zipper-Type                          </td><td>Helix-Loop-Helix                     </td></tr>\n",
       "\t<tr><td>MA0004.1                             </td><td>Arnt                                 </td><td>Basic helix-loop-helix factors (bHLH)</td><td>PAS domain factors                   </td></tr>\n",
       "\t<tr><td>MA0005.1                             </td><td>AG                                   </td><td>Other Alpha-Helix                    </td><td>MADS                                 </td></tr>\n",
       "\t<tr><td>MA0006.1                             </td><td>Ahr::Arnt                            </td><td>Basic helix-loop-helix factors (bHLH)</td><td>PAS domain factors                   </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|llll}\n",
       " motif & TF & class & family\\\\\n",
       "\\hline\n",
       "\t MA0001.1                              & AGL3                                  & Other Alpha-Helix                     & MADS                                 \\\\\n",
       "\t MA0002.1                              & RUNX1                                 & Ig-fold                               & Runt                                 \\\\\n",
       "\t MA0003.1                              & TFAP2A                                & Zipper-Type                           & Helix-Loop-Helix                     \\\\\n",
       "\t MA0004.1                              & Arnt                                  & Basic helix-loop-helix factors (bHLH) & PAS domain factors                   \\\\\n",
       "\t MA0005.1                              & AG                                    & Other Alpha-Helix                     & MADS                                 \\\\\n",
       "\t MA0006.1                              & Ahr::Arnt                             & Basic helix-loop-helix factors (bHLH) & PAS domain factors                   \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "motif | TF | class | family | \n",
       "|---|---|---|---|---|---|\n",
       "| MA0001.1                              | AGL3                                  | Other Alpha-Helix                     | MADS                                  | \n",
       "| MA0002.1                              | RUNX1                                 | Ig-fold                               | Runt                                  | \n",
       "| MA0003.1                              | TFAP2A                                | Zipper-Type                           | Helix-Loop-Helix                      | \n",
       "| MA0004.1                              | Arnt                                  | Basic helix-loop-helix factors (bHLH) | PAS domain factors                    | \n",
       "| MA0005.1                              | AG                                    | Other Alpha-Helix                     | MADS                                  | \n",
       "| MA0006.1                              | Ahr::Arnt                             | Basic helix-loop-helix factors (bHLH) | PAS domain factors                    | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "  motif    TF        class                                 family            \n",
       "1 MA0001.1 AGL3      Other Alpha-Helix                     MADS              \n",
       "2 MA0002.1 RUNX1     Ig-fold                               Runt              \n",
       "3 MA0003.1 TFAP2A    Zipper-Type                           Helix-Loop-Helix  \n",
       "4 MA0004.1 Arnt      Basic helix-loop-helix factors (bHLH) PAS domain factors\n",
       "5 MA0005.1 AG        Other Alpha-Helix                     MADS              \n",
       "6 MA0006.1 Ahr::Arnt Basic helix-loop-helix factors (bHLH) PAS domain factors"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "load(\"Rdata_files/motif_class_pairs.Rdata\")\n",
    "head(motif.class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#load(\"/ssd/mrichard/data/joined.annotated.9.Rdata\")\n",
    "system.time(all.TF.df.fimo.hint.well.annotated <- read.fst(\"/ssd/mrichard/data/joined.annotated.ALL.fst\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've got our fully-annotated dataset all together. We also have an `Rdata` file with motif-class pairings that we'll use somewhere later. Now we want to actually build a model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Only keep rows where hint or wellington had a hit\n",
    "\n",
    "* We have column names with spaces; use the `make.names` function to make them syntatically valid\n",
    "* We're going to pull places where we have at least one of:\n",
    "    * There's a ChipSeq positive\n",
    "    * There's a HINT hit\n",
    "    * There's a Wellington hit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "colnames(all.TF.df.fimo.hint.well.annotated) <- make.names(colnames(all.TF.df.fimo.hint.well.annotated), unique=TRUE)\n",
    "\n",
    "#all.TF.df.fimo.hint.well.annotated %>%\n",
    "#    filter(h_frac_16 > 0 | w_frac_16 > 0 | h_frac_20 > 0 | w_frac_20 > 0 | cs_hit > 0) ->\n",
    "#    df_only_footprint_hits\n",
    "df_only_footprint_hits <- all.TF.df.fimo.hint.well.annotated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our modeling, we also don't need any info on:\n",
    "\n",
    "* Motif location (start/end/strand/chromosome)\n",
    "* Motif name\n",
    "* Sequence\n",
    "* P-value of the FIMO hit\n",
    "\n",
    "So we'll get rid of these. We'll also divide into 3 datasets:\n",
    "\n",
    "1. Validation: Just chromosomes 2 and 4\n",
    "2. Testing: Just chromosomes 1, 3, and 5\n",
    "3. Training: All the other chromosomes\n",
    "\n",
    "Then we'll get rid of the original dataset and the intermediate dataset with only hits. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cols_to_drop <- c('motifname', 'chrom', 'start', 'endpos', 'strand', 'pval', 'sequence','loc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "“Unknown variables: `start`, `endpos`, `pval`, `sequence`”Warning message:\n",
      "“Unknown variables: `start`, `endpos`, `pval`, `sequence`”Warning message:\n",
      "“Unknown variables: `start`, `endpos`, `pval`, `sequence`”"
     ]
    }
   ],
   "source": [
    "df_only_footprint_hits %>%\n",
    "    filter(chrom %in% c(\"2\",\"4\")) %>%\n",
    "    select(-one_of(cols_to_drop)) ->\n",
    "    val_df\n",
    "\n",
    "df_only_footprint_hits %>%\n",
    "    filter(chrom %in% c(\"1\",\"3\",\"5\")) %>%\n",
    "    select(-one_of(cols_to_drop)) ->\n",
    "    test_df\n",
    "\n",
    "df_only_footprint_hits %>%\n",
    "    filter(!(chrom %in% c(\"1\",\"2\",\"3\",\"4\",\"5\"))) %>%\n",
    "    select(-one_of(cols_to_drop)) ->\n",
    "    train_df\n",
    "\n",
    "remove(all.TF.df.fimo.hint.well.annotated, df_only_footprint_hits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purposes of our model-building, we'll turn each dataset into a matrix, which is the input format we need. \n",
    "\n",
    "We'll also separate out the ChipSeq hit, which is our Y (response) and everything else, which serve as our predictors. This ends up with nearly 2 million points in the training set, and a bit over half a million points in the testing set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val_df %>% \n",
    "    select(-cs_hit) %>%\n",
    "    as.matrix ->\n",
    "    X_val\n",
    "\n",
    "val_df %>% \n",
    "    select(cs_hit) %>%\n",
    "    as.matrix ->\n",
    "    y_val\n",
    "\n",
    "test_df %>% \n",
    "    select(-cs_hit) %>%\n",
    "    as.matrix ->\n",
    "    X_test\n",
    "\n",
    "test_df %>% \n",
    "    select(cs_hit) %>%\n",
    "    as.matrix ->\n",
    "    y_test\n",
    "\n",
    "train_df %>% \n",
    "    select(-cs_hit) %>%\n",
    "    as.matrix ->\n",
    "    X_train\n",
    "\n",
    "train_df %>% \n",
    "    select(cs_hit) %>%\n",
    "    as.matrix ->\n",
    "    y_train\n",
    "\n",
    "remove(val_df, test_df, train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>68950307</li>\n",
       "\t<li>24</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 68950307\n",
       "\\item 24\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 68950307\n",
       "2. 24\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] 68950307       24"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>21595414</li>\n",
       "\t<li>24</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 21595414\n",
       "\\item 24\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 21595414\n",
       "2. 24\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] 21595414       24"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>68950307</li>\n",
       "\t<li>1</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 68950307\n",
       "\\item 1\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 68950307\n",
       "2. 1\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] 68950307        1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>21595414</li>\n",
       "\t<li>1</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 21595414\n",
       "\\item 1\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 21595414\n",
       "2. 1\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] 21595414        1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dim(X_train)\n",
    "dim(X_test)\n",
    "dim(y_train)\n",
    "dim(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## gbdt\n",
    "\n",
    "Here's where we actually do the machine learning! We're going to use the `xgboost` package, which is \"Extreme Gradient Boosting\". It's a technique where you're progressively adding things to your model to \"boost\" its performance, at least in my recollection. Seems like the way we're boosting it is by adding trees...I'll have to look more at this. It looks like you can also boost with linear models, but we're using the trees, which are the default. \n",
    "\n",
    "We specify a handful of parameters that do the following (in order):\n",
    "\n",
    "* Set the objective to \"binary:logistic\", telling it to do classification instead of regression or \"ranking\"\n",
    "* Set the maximum tree depth to 7 rather than the default, 6.\n",
    "* Set the contribution of each additional tree to 0.005; this is conservative, it's to prevent overfitting, so we have to add more things, but each particular things adds less to the overall model\n",
    "* Set the evaluation metric to Area Under the Curve; essentially, you do a ROC curve and try to maximize area underneath it. \n",
    "    * Note that you can set multiple metrics; things like error or cumulative gain\n",
    "    \n",
    "Other than the `param` list, we also specify:\n",
    "\n",
    "* The data are our training set\n",
    "* The real labels are our training response\n",
    "* We are permitting 200 rounds of iterations; I'm guessing this is a large number, based on our small eta number\n",
    "* Verbose is FALSE, so it won't print output; it could also be \"1\" or \"2\" for different output amounts\n",
    "* Missing is NA, so consider missing values to be NA; could be 0, but this is the default, as well as our value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note: This takes quite a while (several hours) with our enlarged dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:42:38] Tree method is automatically selected to be 'approx' for faster speed. to use old behavior(exact greedy algorithm on single machine), set tree_method to 'exact'\n"
     ]
    }
   ],
   "source": [
    "param <- list(\"objective\" = \"binary:logistic\",\n",
    "          \"max.depth\" = 7,\n",
    "          \"eta\" = 0.005,\n",
    "          \"eval.metric\" = \"auc\"\n",
    "          )\n",
    "\n",
    "gbdt_medium <- xgboost(\n",
    "    params = param,\n",
    "    data = X_train,\n",
    "    label = y_train,\n",
    "    nround = 200,\n",
    "    verbose = FALSE,\n",
    "    missing = NA\n",
    ")\n",
    "\n",
    "gbdt_medium$Model.Name <- \"trees with classes\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "TRUE"
      ],
      "text/latex": [
       "TRUE"
      ],
      "text/markdown": [
       "TRUE"
      ],
      "text/plain": [
       "[1] TRUE"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "xgb.save(gbdt_medium, \"saved_models/xgboost_TF_site_predict_joined.ALL.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a gradient-boosted classifier called \"trees with classes\" that we've trained. We'll use the model in a moment, but first we're taking our motif class pairs and using `make.names` so that they match the ones we changed earlier...this will be important in a second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "motif.class$class <- lapply(motif.class$class, make.names, unique=TRUE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're pulling out the \"importance matrix\"; we basically give the names of the features in the first argument and the model in the second argument. That's pretty straightforward, and the matrix is only 18 x 4 (number of features is 18). \n",
    "\n",
    "Given it's pretty small, here it is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>Feature</th><th scope=col>Gain</th><th scope=col>Cover</th><th scope=col>Frequency</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>h_frac_20                            </td><td>7.665870e-01                         </td><td>2.983451e-01                         </td><td>0.043300524                          </td></tr>\n",
       "\t<tr><td>h_max_score_20                       </td><td>1.431956e-01                         </td><td>1.487476e-01                         </td><td>0.081600254                          </td></tr>\n",
       "\t<tr><td>h_frac_16                            </td><td>2.697940e-02                         </td><td>1.259798e-01                         </td><td>0.047904429                          </td></tr>\n",
       "\t<tr><td>Rel.homology.region..RHR..factors    </td><td>9.959139e-03                         </td><td>1.404630e-01                         </td><td>0.075964439                          </td></tr>\n",
       "\t<tr><td>gc_content                           </td><td>7.818534e-03                         </td><td>8.841575e-04                         </td><td>0.130536593                          </td></tr>\n",
       "\t<tr><td>motifscore                           </td><td>7.677917e-03                         </td><td>4.005744e-03                         </td><td>0.162128909                          </td></tr>\n",
       "\t<tr><td>Zipper.Type                          </td><td>7.542555e-03                         </td><td>1.388411e-01                         </td><td>0.067153516                          </td></tr>\n",
       "\t<tr><td>Tryptophan.cluster.factors           </td><td>7.436103e-03                         </td><td>1.394114e-03                         </td><td>0.029766630                          </td></tr>\n",
       "\t<tr><td>w_frac_20                            </td><td>4.586724e-03                         </td><td>7.701234e-03                         </td><td>0.068066360                          </td></tr>\n",
       "\t<tr><td>Fork.head...winged.helix.factors     </td><td>4.003650e-03                         </td><td>1.051529e-03                         </td><td>0.028456898                          </td></tr>\n",
       "\t<tr><td>asinh_tss_dist                       </td><td>2.715694e-03                         </td><td>4.924565e-03                         </td><td>0.056437530                          </td></tr>\n",
       "\t<tr><td>w_frac_16                            </td><td>2.201464e-03                         </td><td>6.624534e-04                         </td><td>0.029369741                          </td></tr>\n",
       "\t<tr><td>Winged.Helix.Turn.Helix              </td><td>2.046738e-03                         </td><td>7.983546e-04                         </td><td>0.016510557                          </td></tr>\n",
       "\t<tr><td>Ig.fold                              </td><td>1.933437e-03                         </td><td>6.612058e-04                         </td><td>0.014248293                          </td></tr>\n",
       "\t<tr><td>w_min_score_20                       </td><td>1.586219e-03                         </td><td>1.126873e-01                         </td><td>0.062073345                          </td></tr>\n",
       "\t<tr><td>h_max_score_16                       </td><td>1.519760e-03                         </td><td>1.210271e-02                         </td><td>0.046197809                          </td></tr>\n",
       "\t<tr><td>Basic.helix.loop.helix.factors..bHLH.</td><td>1.012987e-03                         </td><td>3.484607e-04                         </td><td>0.010517542                          </td></tr>\n",
       "\t<tr><td>w_min_score_16                       </td><td>7.639565e-04                         </td><td>2.671274e-04                         </td><td>0.023416415                          </td></tr>\n",
       "\t<tr><td>Zinc.coordinating                    </td><td>1.873308e-04                         </td><td>7.711375e-05                         </td><td>0.001547865                          </td></tr>\n",
       "\t<tr><td>C2H2.zinc.finger.factors             </td><td>1.669758e-04                         </td><td>1.110858e-05                         </td><td>0.001865375                          </td></tr>\n",
       "\t<tr><td>Basic.leucine.zipper.factors..bZIP.  </td><td>7.881909e-05                         </td><td>4.631996e-05                         </td><td>0.002936974                          </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|llll}\n",
       " Feature & Gain & Cover & Frequency\\\\\n",
       "\\hline\n",
       "\t h\\_frac\\_20                             & 7.665870e-01                              & 2.983451e-01                              & 0.043300524                              \\\\\n",
       "\t h\\_max\\_score\\_20                        & 1.431956e-01                                & 1.487476e-01                                & 0.081600254                                \\\\\n",
       "\t h\\_frac\\_16                             & 2.697940e-02                              & 1.259798e-01                              & 0.047904429                              \\\\\n",
       "\t Rel.homology.region..RHR..factors     & 9.959139e-03                          & 1.404630e-01                          & 0.075964439                          \\\\\n",
       "\t gc\\_content                            & 7.818534e-03                            & 8.841575e-04                            & 0.130536593                            \\\\\n",
       "\t motifscore                            & 7.677917e-03                          & 4.005744e-03                          & 0.162128909                          \\\\\n",
       "\t Zipper.Type                           & 7.542555e-03                          & 1.388411e-01                          & 0.067153516                          \\\\\n",
       "\t Tryptophan.cluster.factors            & 7.436103e-03                          & 1.394114e-03                          & 0.029766630                          \\\\\n",
       "\t w\\_frac\\_20                             & 4.586724e-03                              & 7.701234e-03                              & 0.068066360                              \\\\\n",
       "\t Fork.head...winged.helix.factors      & 4.003650e-03                          & 1.051529e-03                          & 0.028456898                          \\\\\n",
       "\t asinh\\_tss\\_dist                        & 2.715694e-03                              & 4.924565e-03                              & 0.056437530                              \\\\\n",
       "\t w\\_frac\\_16                             & 2.201464e-03                              & 6.624534e-04                              & 0.029369741                              \\\\\n",
       "\t Winged.Helix.Turn.Helix               & 2.046738e-03                          & 7.983546e-04                          & 0.016510557                          \\\\\n",
       "\t Ig.fold                               & 1.933437e-03                          & 6.612058e-04                          & 0.014248293                          \\\\\n",
       "\t w\\_min\\_score\\_20                        & 1.586219e-03                                & 1.126873e-01                                & 0.062073345                                \\\\\n",
       "\t h\\_max\\_score\\_16                        & 1.519760e-03                                & 1.210271e-02                                & 0.046197809                                \\\\\n",
       "\t Basic.helix.loop.helix.factors..bHLH. & 1.012987e-03                          & 3.484607e-04                          & 0.010517542                          \\\\\n",
       "\t w\\_min\\_score\\_16                        & 7.639565e-04                                & 2.671274e-04                                & 0.023416415                                \\\\\n",
       "\t Zinc.coordinating                     & 1.873308e-04                          & 7.711375e-05                          & 0.001547865                          \\\\\n",
       "\t C2H2.zinc.finger.factors              & 1.669758e-04                          & 1.110858e-05                          & 0.001865375                          \\\\\n",
       "\t Basic.leucine.zipper.factors..bZIP.   & 7.881909e-05                          & 4.631996e-05                          & 0.002936974                          \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "Feature | Gain | Cover | Frequency | \n",
       "|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n",
       "| h_frac_20                             | 7.665870e-01                          | 2.983451e-01                          | 0.043300524                           | \n",
       "| h_max_score_20                        | 1.431956e-01                          | 1.487476e-01                          | 0.081600254                           | \n",
       "| h_frac_16                             | 2.697940e-02                          | 1.259798e-01                          | 0.047904429                           | \n",
       "| Rel.homology.region..RHR..factors     | 9.959139e-03                          | 1.404630e-01                          | 0.075964439                           | \n",
       "| gc_content                            | 7.818534e-03                          | 8.841575e-04                          | 0.130536593                           | \n",
       "| motifscore                            | 7.677917e-03                          | 4.005744e-03                          | 0.162128909                           | \n",
       "| Zipper.Type                           | 7.542555e-03                          | 1.388411e-01                          | 0.067153516                           | \n",
       "| Tryptophan.cluster.factors            | 7.436103e-03                          | 1.394114e-03                          | 0.029766630                           | \n",
       "| w_frac_20                             | 4.586724e-03                          | 7.701234e-03                          | 0.068066360                           | \n",
       "| Fork.head...winged.helix.factors      | 4.003650e-03                          | 1.051529e-03                          | 0.028456898                           | \n",
       "| asinh_tss_dist                        | 2.715694e-03                          | 4.924565e-03                          | 0.056437530                           | \n",
       "| w_frac_16                             | 2.201464e-03                          | 6.624534e-04                          | 0.029369741                           | \n",
       "| Winged.Helix.Turn.Helix               | 2.046738e-03                          | 7.983546e-04                          | 0.016510557                           | \n",
       "| Ig.fold                               | 1.933437e-03                          | 6.612058e-04                          | 0.014248293                           | \n",
       "| w_min_score_20                        | 1.586219e-03                          | 1.126873e-01                          | 0.062073345                           | \n",
       "| h_max_score_16                        | 1.519760e-03                          | 1.210271e-02                          | 0.046197809                           | \n",
       "| Basic.helix.loop.helix.factors..bHLH. | 1.012987e-03                          | 3.484607e-04                          | 0.010517542                           | \n",
       "| w_min_score_16                        | 7.639565e-04                          | 2.671274e-04                          | 0.023416415                           | \n",
       "| Zinc.coordinating                     | 1.873308e-04                          | 7.711375e-05                          | 0.001547865                           | \n",
       "| C2H2.zinc.finger.factors              | 1.669758e-04                          | 1.110858e-05                          | 0.001865375                           | \n",
       "| Basic.leucine.zipper.factors..bZIP.   | 7.881909e-05                          | 4.631996e-05                          | 0.002936974                           | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "   Feature                               Gain         Cover        Frequency  \n",
       "1  h_frac_20                             7.665870e-01 2.983451e-01 0.043300524\n",
       "2  h_max_score_20                        1.431956e-01 1.487476e-01 0.081600254\n",
       "3  h_frac_16                             2.697940e-02 1.259798e-01 0.047904429\n",
       "4  Rel.homology.region..RHR..factors     9.959139e-03 1.404630e-01 0.075964439\n",
       "5  gc_content                            7.818534e-03 8.841575e-04 0.130536593\n",
       "6  motifscore                            7.677917e-03 4.005744e-03 0.162128909\n",
       "7  Zipper.Type                           7.542555e-03 1.388411e-01 0.067153516\n",
       "8  Tryptophan.cluster.factors            7.436103e-03 1.394114e-03 0.029766630\n",
       "9  w_frac_20                             4.586724e-03 7.701234e-03 0.068066360\n",
       "10 Fork.head...winged.helix.factors      4.003650e-03 1.051529e-03 0.028456898\n",
       "11 asinh_tss_dist                        2.715694e-03 4.924565e-03 0.056437530\n",
       "12 w_frac_16                             2.201464e-03 6.624534e-04 0.029369741\n",
       "13 Winged.Helix.Turn.Helix               2.046738e-03 7.983546e-04 0.016510557\n",
       "14 Ig.fold                               1.933437e-03 6.612058e-04 0.014248293\n",
       "15 w_min_score_20                        1.586219e-03 1.126873e-01 0.062073345\n",
       "16 h_max_score_16                        1.519760e-03 1.210271e-02 0.046197809\n",
       "17 Basic.helix.loop.helix.factors..bHLH. 1.012987e-03 3.484607e-04 0.010517542\n",
       "18 w_min_score_16                        7.639565e-04 2.671274e-04 0.023416415\n",
       "19 Zinc.coordinating                     1.873308e-04 7.711375e-05 0.001547865\n",
       "20 C2H2.zinc.finger.factors              1.669758e-04 1.110858e-05 0.001865375\n",
       "21 Basic.leucine.zipper.factors..bZIP.   7.881909e-05 4.631996e-05 0.002936974"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "importance_matrix <- xgb.importance(colnames(X_train),model=gbdt_medium)\n",
    "importance_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we're seeing here is the following:\n",
    "\n",
    "* Gain: contribution of a feature to the model (higher is better, and it's sorted by this)\n",
    "* Cover: Number of the observation related to the feature\n",
    "    * I'm not exactly sure what this means, but I don't think we're using it much\n",
    "* Weight (Frequency?): percentage of time the feature pops up in trees\n",
    "\n",
    "So we're most interested in the Gain, which basically tells us how important each feature is. We're going to turn this matrix into a data frame, then we'll:\n",
    "\n",
    "1. Separate out features that are TF classes and those that are not into 2 separate data frames\n",
    "2. Take the TF class data frame and do the following to make it into 1 feature, \"TF_Class\"\n",
    "    1. Select only the non-Feature columns, so the numeric ones that can be added\n",
    "    2. Sum those columns, so the gains are additive\n",
    "    3. Make them into a list\n",
    "    4. Take away their names so you can add \"TF_Class\" to them via concatenation\n",
    "    5. Add the correct names to each entry\n",
    "    6. Add that list as a row to the data frame\n",
    "\n",
    "Now we have the same importance matrix, but all the TF-Class features are added into one row. ***It's interesting to me that you can just do that...something to ask Rory about perhaps***.\n",
    "\n",
    "Now that we have that matrix, we'll make a barplot of the Gain variable to show which features had most gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAC91BMVEUAAAABAQECAgIDAwME\nBAQFBQUGBgYHBwcICAgJCQkKCgoLCwsMDAwNDQ0ODg4PDw8QEBARERESEhITExMUFBQVFRUW\nFhYXFxcYGBgZGRkaGhobGxscHBwdHR0fHx8gICAhISEiIiIjIyMkJCQlJSUmJiYnJycoKCgp\nKSkqKiorKyssLCwtLS0uLi4vLy8wMDAxMTEyMjIzMzM0NDQ1NTU2NjY3Nzc4ODg5OTk6Ojo7\nOzs8PDw9PT0/Pz9AQEBBQUFCQkJDQ0NFRUVGRkZHR0dISEhJSUlKSkpLS0tMTExNTU1OTk5P\nT09QUFBRUVFSUlJTU1NUVFRVVVVWVlZXV1dYWFhZWVlaWlpbW1tcXFxdXV1eXl5fX19gYGBh\nYWFiYmJjY2NkZGRlZWVmZmZnZ2doaGhpaWlqampra2tsbGxtbW1ubm5vb29wcHBxcXFycnJz\nc3N0dHR1dXV2dnZ3d3d4eHh5eXl6enp7e3t8fHx9fX1+fn5/f3+AgICBgYGCgoKDg4OEhISF\nhYWGhoaHh4eIiIiJiYmKioqLi4uMjIyNjY2Ojo6Pj4+QkJCRkZGSkpKTk5OUlJSVlZWWlpaX\nl5eYmJiZmZmampqbm5ucnJydnZ2enp6fn5+goKChoaGioqKjo6OkpKSlpaWmpqanp6eoqKip\nqamqqqqrq6usrKytra2urq6vr6+wsLCxsbGysrKzs7O0tLS1tbW2tra3t7e4uLi5ubm6urq7\nu7u8vLy9vb2+vr6/v7/AwMDBwcHCwsLDw8PExMTFxcXGxsbHx8fIyMjJycnKysrLy8vMzMzN\nzc3Ozs7Pz8/Q0NDR0dHS0tLT09PU1NTV1dXW1tbX19fY2NjZ2dna2trb29vc3Nzd3d3e3t7f\n39/g4ODh4eHi4uLj4+Pk5OTl5eXm5ubn5+fo6Ojp6enq6urr6+vs7Ozt7e3u7u7v7+/w8PDx\n8fHy8vLz8/P09PT19fX29vb39/f4+Pj5+fn6+vr7+/v8/Pz9/f3+/v7///+3cnWnAAAACXBI\nWXMAABJ0AAASdAHeZh94AAAgAElEQVR4nOy9e3QUyXrgWb5jj+1rr9f22nd9jl/jmVnv2GeO\nvTu7PrM7s7Z3vd6ZO19JQoBAPIWAC0KNJMRLQLe4BgQtWtDypYGmeTb30nSracQbIdE076du\nc8VDqOkWLyEQpferJFTK/GO/yHdWZUZmoqqiVHy/c6AiIyMis1Lxq8iIjMz0iQRBjBjf694B\ngkgGSCSCiAIkEkFEARKJIKIAiUQQUYBEIogoQCIRRBQgkQgiCpBIBBEFSCSCiAIkEkFEARKJ\nIKIAiUQQUYBEIogoQCIRRBQgkQgiCpBIBBEFSCSCiAIkEkFEARKJIKIAiUQQUYBEIogoQCIR\nRBQgkUYVgiB4zOC1/Dep+GiWTyKNKoKBoKf0gz2ekg8HujylF9u8Je8MeEvf/dJT8v7AgKf0\nA72ekocC3fYrSaRRBYnEg0QiXEIi8SCRCJeQSDxIJMIlJBIPEolwCYnEg0QiXEIi8SCRCJeQ\nSDxIJMIlwYlEHDEffBIpeSCR4or54JNIyQOJFFfMB59ESh5IpLhiPvgkUvJAIsUV88EnkZIH\nEimumA8+iZQ8kEhxxXzwSaTkgUSKK+aDTyIlDyRSXDEffBIpeSCR4or54L+hIt0HgBb3yZ+X\nzR2TVRe73YkOJFJcMR98EskFdWMxOVyOwX4MXi7LnZQ6uWBHoym6Yeu8jPE5m+94K4xEiivm\ng08iOTM0B2Ik0tczQGWd/nfo2aBGlnqaJkoixRXzwSeRnLmDiTe0hLw+hcaZY2BgpjpZui9f\nj8zr81AciRRXzAefRHLmFED6YAx24ud+3Iu5h27fr5aaoPwhOboEw1MrfnHzsykYKPFQHokU\nV8wHn0RyZj/A7Bjsg8Banm0hKXyHneMdkYK3MDS/k4U6cjF4232BJFJcMR98EskZFGlODPbh\nHu7DavV88S62TjOkhaUAaU1y5ONUgKXuCySR4or54JNIzsRIpC24Dw+1pXJceoCfbfj5gRq5\nCRfc32dKIsUV88F/s0US7tdUHL/Z7pA4QqT+ixXVSnDwVvWBygsNYQcxVHeq4sQNhxu/iwFm\n6QMYNbhHF/DzKH7Wq5F3ceGYw97pkEhxJexP/iaLVCuPa/vLOU8LqNVG0C5L43eo1MUpqlmP\ny9Lldf6V9/Uswd2ZUmR6KbfRmwewSl/6BjMcx891AOOH1cjQOIC1rr8UiRRXzAf/TRbpE82R\nCQ9tU0aKdJYNtkkiVafqA9X+M2qOR1laZPp5zj5kAGzVl65h8huipNdyPXYZQK7rL0UixRXz\nwX+DRdoNkPHunk1saAwKbC8S1RcWTgcYU1hYeEsWqVGyh4l0V3Ilf/XKXKaWOkLQNBkXUheX\n5LPGKrXWfh96e3sNg+o7MfUz/Htg6eV67Ebc9HBkVmtIpLhiPvhvsEhYY9nlTuHiGAzyHgWl\n95FQpOxcSNlw/RtMLyxgw9fSQHXzGgxul5IML2QFsxkJXWyoIMflZdxOPImbi2lbMM8OPXq7\nw66ZIJHiivngv8kivadU8lMYvsJJbBIJYJIyL46Nr61Xihh6C6BQClUzu5SM2J7AVVf7M7gY\nk57BwEP83G/aNDx2VYJIIsUZ88F/g0VKe6Es9OLCIU7iMJHUWeB1GL6npsHzxJlSoAB7XOpw\nXV8awC43u/NiCZZVyK7NsotLR/UVbBLRNzaZejvDaH/dVevNwnzwOwJtnbZnH8kt0iZtaaoX\nkZap0d/u3r0npC5U4kkf+2wCoztvAxQ778zwyQms1yVNUL2JobP6qnMGccPpDITzuqvWm0XE\n4Q/Y9maTW6QqbSnbi0hfWKbZrIjEzuyatNiWe/caLZMbqXuLtXO58lA5E+mcvu4MLn5tk08I\np/91V603C/PBHwp023eHk1sk/Zfek0hWDUTf4VRFpC0AKSGLFLZ0rJMGz3cpD7KuVy4nKZgu\nzzpBfaS4Yj74b3AfqVlb8iTSA9O67vqaPSVz2PC3LBJqkeVlRy5K124LNVke4NJnpk2D/TWu\nMEikuGI++G+wSPq0A08iPddXDBxc5NcvyUoirQBY5H43espYxlnn9ZMCNvy9U0+wAzzMCSSR\n4or54JNIokeR9Gx31Ptbxxbs26aItBxgoeu9aJvLrugeHDJEDZsvyJYDpLo+UySR4or54JNI\n4quK9CiDTV1Yuvfac2xQDikirQWY5nYn+thUvyXN5sgc00BfMUCO2+JIpPhiPvgkkviqIq3G\nhZJnysIhfbDBr7cwoWAwaD+Ws57NgQhvb7CTlallETJp0mrCYj74JJL4iiJ1YfdouXblQBWJ\n3Q2hTwSv5J2ZNWDSsgjNThiHFx7jwkmn76JBIsUV88EnkcRXFIl5cEFLs1cRqZnZocWuAJhr\nW+y7AJMiD/4LNp1WXdiDC63cL2KERIor5oNPIomvKNJ1U9uzXBFJXAKQog7sPUwF+NCu1MFx\nAFss4pcBTOiUg114ZrfC4ZsYIJHiivngk0jiK4pUr0wzlahWh7/FSxjKl99y3JeH4Qa7Uhtx\nZfEOE5KAbArfKul8cIj1wujhJ4mK+eCTSOIrijQwBiBPno8gnGT3KE2Sw6z2TzvaMvj09HTg\nPU7rEkQgPVlVYFMdltYF+36+CAOlHr4UiRRXzAefRBJfddSOPa6k4GrryxdnsMazofDjA2y8\nrjvPoMYs+6N70EYksX+eHjPf4bkPJkikuGI++G+oSE1FRUX6M09Ki4oucBLbiNQ2Ta/wJQ+l\nD2mWXE+JFr2U81yVrXYiib1aAaVeHrRKIsUX88F/Q0XyxLPaWqWn0l5VVdWvxT+dr9T3KVWC\n8I4mEnZzNrAZdBmrrrziU46F2xtzxo/PKb/rLT+JFFfMB59EGgHDV3atmrfovbPs/KtnZ2FB\nmT6yEGzpif6zwh0gkeKK+eCTSMkDiRRXzAefREoeSKS4Yj74JJJK0IaRnqHFqlyLLb3uqvVm\nYT74JJJKduQomoSHN2TGtdxISKS4Yj74JJIKiUR4wnzwSaTkgUSKK+aDTyIlDyRSXDEffBIp\neSCR4or54JNIyUMw4GVmnigO9nhKPhzw9I51UXT/ijSJTtcPOZfpfukpeX9gwFP6gV5PyUmk\n5IFE4kEiES4hkXiQSIRLSCQeJBLhEhKJB4lEuIRE4kEiES6JzvC3bfEkEg8SKXkgkXiQSIRL\nSCQeJBLhEhKJB4lEuIRE4kEiES4hkXiQSIRLSCQeJBLhEhKJB4lEuIRE4kEiES4hkXgkp0j3\nY/LQgjccEokHiZTMDF4uy52UOrlgR6MpumHrvIzxOZvveCuMROJBIiUxX8/QHiq0Tv879GzQ\nHqLvqeqSSDxIpOTlmPHxXDPVeteXr0fmeXkdBYnEg0RKWn7ux6Mw99Dt+9VSE5SvvA2dvdRl\nasUvbn42BXgvKouEROJBIiUrAmt5tskvPb/DzvGOSMFbGJovvUS2Ixdew6svbYsnkXiQSK+N\ne3gMVqtPAL+LrdMMaWEpQFqTHPk4FWCp+wJJJB4kUrKyBY/BQ22JvUjzAX624ecHauQmXHBf\nHUkkHkktknC/puL4Tc77IQ28vH6souaBHB64drTifOOwcfXgreoDlRcaON+Gy+Djq5VHbzSF\nTJGhulMVJ26Y798WGk5/XvnV44h3SfRfrKhWw20XjnxxqTEUniSMYoBZejE1eETY+zeP4me9\nGnkXF465/g4kEo9kFql2jjQ05S/nHkJMmy0KBzKltAufieLQ3nFSOP+eluZxWbo8zOVfeV+O\n2Yxh7QV6rJZWRxSs07FZLhE7+Xr1D+6WN5leqredg/unyglnV6ka35HeMHtxivai2bolyjDc\nMf5fbh7AKn3pG5DfnLkOYLz2CxHC3VrLLcQIicQjiUX6RBvlnfCQnzY7tFpNmvm8r1ANa6ZU\np+ojxv4zUlR/NsA8ZRysfQLAP3HeR1Q/Uc9eNKhEPsrS4tLPK3HPZusJFyuPDpFEOssG4CSR\nhjfrSXK4VSMDYKu+dA3T3xAlvZbrscsAcnllmCCReCSvSLsBMt7ds4kNTUEBp5pj2hnYWfhR\n+bYilnQJ9sanl+4oZlX3Lbn9uCvV9vzVK3NZpNJVv4nBn8klrEX/OOePgfGYNDVv9QpJHaVy\nN01mkYtL8llTl1orJ5TeYz5n1Y+lhPPlcz4mUqMkMhNJKGOhsYtWz2WfWU85x6C3t3dQX9qJ\nybG1DWFJ5XrsRoAxw5FZrSGReCSvSFhj2OVG4eIYDHIOIqb1Q+pRpto1ueXZxfypnwRKZ11Y\ngKFt0ohx8xoMbpfzYdOQKq2/hHHnrcuWeB/Xb2B/deECWpMiFTS8kO0fqzldrMefw7YuFDN9\nGjAo1GZr20GRsnMhZcP1b9h3OIXxE8+xul8/C4NL3b6XrxNP4uZi4hbMtEOP3s4/NGZIJB5J\nLNJ7Si1jle+KQ9rTcngHC++RwydVP9owsF4pa+gtgEI5yE7uClC5HuzWlHL2RcBzrCLlZ591\n9c+xQDVzU0mAzQJcxc8brOnsl+NaZ6Dez1noDtulScpcuT4sa0arEmYN6AXHQyExuBjTnsHA\nQ/zcr8fvx8XHNnleDoTRExWRwkvVCAY6bNdZ0uoteXvAW/qOPk/JuwM9ntL3dHlK3o9Hx/ZX\nM7Yipb1QFnpx4RA/7WJlH9lJXFZQz1bBAnUY0MYd8IRxphL8BcYfkBqcqbzBvE5MdlAJBzD8\nOQsUYMdNHa7rS8NGED9XYROn1errmPJjFpBEqlNij2D4mpqkGfMt4mxY5wUbnyhk7Sy7uHRU\nX8EmEX1jt9+BcKIiUkSphFtsT8JjK9ImbWmqo0iHlXAHhjerKyYpP97f7t69Rxtsq2RjfAp4\ncpf+9OfGum1FEBPsVcJCKBRih6MJZHdk3gYoxlYDzyvf1eKEXGV0gYm0TI1dYAiL4kf4a2Ho\nBtkxfHIC62FJJ06sZ3dWX3XOIGk4A/1hdEdFpPBSNfoC7bbrLGn1lrw94C19R4+n5F2Bbk/p\nuzs9Je8NdPS/phapSlvKdhRJrUys0h83ZNsfkXqzQaQgpliSbeq9W5EFMDastrIzuyZtqeXe\nvUa5NTynJ/kpLrLTaCbSF0rcQArAJ3oS1jlzvhWi7i3WpuXKY+w3zRs5g4tfO5agQH0kHsnb\nR9LrrrNI6hkVE+mSIVu4SH2HUw0iSSd32GlxmEL9MUu0sto4rrcFICX8gupxTHVfXzwN8kS4\nO4avUm+u9824+BV/22LHOrZ1/66XegnH9bWmy7NOkEg8klekZm3JWaQnSpiJdNmQTRepu75m\nT8kcNvxtEEmahgO/cNiZlwsk32DWxhr1YGDtzgpP9ikmMVSlOmVXmEgPlLgrEMFJ/rYvShd9\nCzVZHuDSZ/rq/WCcRuQAicQjeUXS5wuMWKSBg4v8et01iNSG0bOc5uqIQxXKvAhIXSk3Liss\nhgl2Yo/LcBb8DOTZEkyk50pcdaRIB8OLMdJTJgl8Xi+WDX/v1BPsAA9zEkkkHiSSs0h31BtN\nxxbs22YSiU0F1XowHLpPr1Fd2sq8Ww6wMDwNngH6DU6y8Qg2KH/H8FXY6d7kKSZ4IrWxq7bp\nB4cMUcPmC7K4/6mOvwMqJBIPEslRpEcZrC1Zuvfac/xlP2QU6bqkRjpvfoHG4K39y1PUNmQt\nwLTwBAfMrUMtyFeXjCJdxbCrjUn0samGS5rNkTnSCKFKMUCO6/JIJB4kkqNIbB5eyTMl0ihS\nN7sUm+lhfkH3PmwQ0oNS38qvNxShYDAoiF+C6TY7NvbA5voZRWoA+9HqSNZj6vLw9gZ7Z5na\n7gqZNGnVFhJJjK5IXdgPWq5dCzOK9B7A9D52usW9FaG/q0s/BhUgDc3VgHGErlI6wXoC+vUs\nUbpXKJVdJDKKxC41GS6nBpubm+1nyjHryiIUP2EcXnjsPFphgETiQSI5icQqpD4TZ68u0kXp\n5Et4G2Acr8OOrU+a9jdplAtjA9dlWooVAHPl5kGfXBucqIxHGEUSFyvT8mQ+AJhq3xi+CzAp\n8uC/wOJ2qwt7cKGVs+tmSCQeJJKTSNdNrcdyTaTOSQDr8fNZOnY7OCd3FcbzMXUqwRKAFHUs\n7iG2Mx+K0kQF/WLpHrWxMIl0Cgw3Pj1L410LHhwHsMUifhnAhE452IXqrrDf8XBIJB4kkpNI\n7BrmGTWuWhv+FkqwtyHVyIMYV2NfPsu/QDkKXTlyH0malJAv33HUl6f0hppQqHHKLVBf4vnk\nZGkynkmkAeyVjVEmJLWzm5e+s90sa/uKd5iQzGXXp1ZJPaeh1UAPP7GHRBKjK9LAGIA8+aAK\nJ9l9FpOk8BnNrxCbgWp/P9IwygOZB+u7+x4cYDcaSZO+BVaJpx1tGXx6ejqoT8VidyKmbv82\n2HurlA3vyVMsTCLJ44SlNzuDDyvY3YIfWGxP4VLkNSdpOpHApjosrQv2/XwR8Keth0Mi8SCR\nHEft2MWigqutL1+cwarHhsKPDwyJbRMAViondI0pAGvsT+7qUo21OU++UaI7zxA3Sz5IobXG\nhMpdg2aRpBNFjaXB8G3pHLQRSeyfp8fM5xQQAYnEIzlFaioqKtLbiNKiIs59O44itU3Ta17J\nQ+njuLAST8PU+zSkDs050ZarmXoBperx6ynRfVB3NbRTcy7jhBIXJpJYo9+3vomnwVY7kcRe\nbculXh60SiJxSU6RvNBZVVWlfqkQhrVae66qSu6CPJ2vVLwpVYLwjiRSNybUuxeDuHRZtKfz\n86JsbLX8czcbb/6p28AEy1h1xdCYPdnOej7+/E861Zh2LLvfWFj3ZwvYhd2Zm+z7Rw4Itzfm\njB+fU37X7fUvGRKJB4nkguEru1bNW/TeWdYC9OwsLChrcMwSQSjQHvmXCbb0RFTmUGuH02MU\nhM4XXs7JogSJxINEIlxCIvEgkQiXkEg83hiRgjZ46ye8zg287s2SSDzeGJGyI0exJKL2iPCY\nb+B1b5ZE4kEijZoNvO7Nkkg83hiRiJFCIvEgkQiXkEg8SCTCJcGAt4tXgz2ekpNIPEik5IFE\n4kEiES4hkXiQSIRLSCQeJBLhEhKJB4lEuIRE4kEiES6JHP7mpyeReJBIbywkEg8SiXAJicSD\nRCJcQiLxIJEIl5BIPEgkwiUkEg8SiXAJicSDRCJcQiLxIJEIl5BIPEgkwiUkEg8SKeY8sb4X\nvFiU3yYWzhOn8ozUgukNzjGFROJBIsUcEskVJBIPEolEcgmJxINEEsW+cyrFWO0PqgvsfWNM\npB3nzHh6sD2J5BoSKXnYFt7gMJF4T993hERyDYmUPJBIHEgkHiSSERKJA4nEg0QyQiJxIJF4\nkEhGoiBSqO5UxYkb2j3fZpEGb1UfqLzQYD7kg4+vVh690RRyinOEROJBIsWREYsU3C2/RTO9\nVHm0t1Gkx2Xp8gC6f+V9LUfH5nFy5NSKEC/OzcZJJA4kUhwZqUiPsrSrTennpRiDSNWGdz77\nzyg56vU3zkLRoH2cG0gkHiRSHBmhSE2TMXnq4pJ81vKk1rIoXaS7kl75q1fm+jGQ1iRFBsaz\nlHmrV0gGbrWNcwWJxINEiiMjE2l4IaYuZ7WtaxOGctirxDSRhAUY2ia9wrl5DQa3S1nex9AG\nVuGECyhfSqddnCtIJB4kUhyxFMnMHfvc1UwVJbwRw1dFg0htGFivvKVv6C2AQhYQMvDkTXmz\n81FMcM4mzh0kEg8SKY6MTKQCgAnqcF1fGsAu0SBSHQbuqSl3A8xkn50YeVCJC2D4c5s4S7pa\nw4kQKSKFmYDD+vDkXtN7S+55d7wWH9vysXjbl92TSF5EagLZHZm35Vmvmkjf7t69RxuCqwTI\nZp9BXLtXiRNCodCwTZwlPR1htEWIFJ7CTHsbf3148kCrp/QdAW/JW72mb/eUvC3g8et6Pzq2\nrwUmkSInrdp3WdiZXZO21HLvXqNod0F2syKSmAUwti5spVWcK+jUjged2sWREQ02bAFIibjs\nYyVS3+FUVaSPWSO3srrduNoqzhUkEg8SKY6MSKR1AFkRkWaRuutr9pTMYcPfikgvF8gnjLM2\n1mh/B6s4V5BIPEikODIikVYALIqINIg0cHCRX+9rySKJQxXKbAdIXamez1nFuYFE4kEixZER\nibQcYGFEpC7SnRmKHWML9m3TRML6cHqN6s3WECfOGRKJB4kUR0Yk0lqAaRGRmkiPMlgLs3Tv\nteeCKB4yiCSyuaz7l6eAYdjbJs4BEokHiRRHRjrY4B/SlkLBYFAwiLQaAyXPlJVhIjG696UC\npAcd4ziQSDxIpDgyIpFqMK0+q7sS25+QLlIXdo+Wa5eEVJH6u7r0w18h57eKcweJxINEiiMj\nEqkZ05ZpSysA5oq6SA34eUFbuVcRCRuxNK06NMpJrOLcQSLxIJHiyMgmrS4BSHmuhB/iSdmH\noi7SdVPTslwRiTU42rjcOXnBKs4dJBIPEimOjEykS5g4X66cfXkYbhB1kerx84yasFod/max\nC5Q/QFeO3B+yinMHicSDRIojIxNJYAMK0462DD49PZ0NLbA4VaSBMQB58l9eOMnu8JvEgsMo\nCmQerO/ue3CA3X20zSbOHSQSDxIpjozwxr7uPP2CK8ySDqw2aleOgYKrrS9fnFkEwIbCjw8M\niWKd4a5ZNK2fpbSKcwWJxINEiiOfzZkz55kx4hxG3HSfv6dEE2CpPFeuoaioSHo0a9s0XY6S\nh9LHcYy+mqlHlyp/Oqs4N5BIPEikUUXdBmZBxqorEVPqn85X3JhSJQjvqCKJnZ8XZacA+Odu\n/kZLahXnAhKJB4k02gi29FjemDJ8ZdeqeYveO8sGD3p2FhaUNahrQoH2iEphFee0YRKJA4lE\nuIRE4kEiES4hkXiQSIlG0AbbG43jtmMkEgcSKdHIBmtaXveOkUg8SKREg0RyCYmkQiKNKkgk\nHiQS4RISiQeJRLiEROJBIhEuCQZcTxSXIJF4kEhvLCQSDxKJcAmJxINEIlxCIvEgkQiXkEg8\nSCTCJUEPI3YMEokHifTGQiLxIJEIl5BIPEgkwiUkEg8SiXAJicSDRCJcQiLxIJEIl5BIPEgk\nwiUkEg8SiXAJicSDRCJcQiLxIJFiwX1vt4Y/L5s7JsvL61xfCyQSDxIpFngTqW4seHoGuEe2\nZeSGxQzXluaOGz9v60NP5ZBIPEikWOBJpKE5EEuRQpnh78FszFEfqPLhkHUeS0gkHiRSLPAk\n0h1MvKElFKvn1l0Nf6Hs+TTD8/aHbXJZQCLxIJFigSeRTgGkD8ZsV7qywkS6NwbAv+Zs/bmf\n+MHLS81JJC4kUizwJNJ+gNmx2hGhrhDMIg1lA4y9KgXvpgOMd3+PEYnEg0SKBV5FmhOTvbh1\neFOudAJnFIm926xGCR/D8FXXxZFIPEikWJAYImkvFDOIJGCDtFDtjr3MANjkujgSiQeJFAsU\nkYT7NRXHb7Y7JI4Qqf9iRbUSHLxVfaDyQkPYQQzVnao4ccPxpMxKpG9x8by2dGLr1v1OpWiQ\nSDxIpFggi1Qrj2v7yzl/wlptAO2yNH6HSl2copr1uCxdXudfeV/PEtwtG5Je6tDofVnFKDSJ\ntA8g1f1rY02QSDxIpFggifSJ5sgE+yufkSKdZYNpkkjVhtcm+8+oOR5laZHp523L1SkzibQC\nz+xe8UuRSDxIpFjARNoNkPHuHrm7X2B7kai+sHA6wJjCwsJbskiNkj1MpLuSK/mrV+YytdKa\n5AxNk3EhdXFJPmusUmud98UsElpY+opfikTiQSLFgvtSg1HO3jcuXByDQd4fUe8joUjZuZCy\n4fo3mF5YgPm2dbL45jUY3C4lGV7ICmZVrmsThnKcL+OaRAqyMsXhS+tmp0+Y9+E9T1+KROJB\nIsUCSaT3lEp+CsNXOIlNIgFMapQX2jC8Xili6C2AQilULZkgs9HV6LVJpCbMUtG8SD03XMv5\n60RAIvEgkWIBEynthbLQiwuHOInDRFJngddhWGsy8DxxphQowB6XOlzXlwawy3FfTCJ9h4X+\nbKre9ZrTapsv2BtGp0mk8LWRdLU7pzHQE/CWvrfVW/K2gLf07d2ekncGujyl7+rwlJwdHduz\nj+QWSb9AM9WLSMvU6G93794TUhcqFRtYi7JLy/k2QLHjvphEYv0u7IRN3nHm9oly1vVaErLL\n1xkIxyRSxFoi1tjOi0xukaq0pWwvIn1hmWazYgM7s2vSYlvu3Wt03BeTSF9L7dA6+STqLhv/\nq7bLN/QyjF6TSOFrI+nvck5jYCDQ4Sn9yzZvyTsC3tJ39ntK3hPo85S+r9tT8oFAp/2pZnKL\npN+o50kkq/v7+g6nKjZsAUixbUOsMYn0C+bRYvUcgW1vjutZ59RH4kF9pFjARGrWljyJ9MC0\nrru+Zk/JHHYOJtuwDiDL476YRGrAglIeaYslpv10gETiQSLFAvNcO08iPddXDBxc5NfHBSQb\nVgAs8rgvJpEeYkGG+2VrcPFLtwWRSDxIpFgwApH0bHdmKAqNLdi3TbFhufeJCSaR2sF0Qbbe\ntlNmAYnEg0SKBdEQ6VEGG2Fbuvfac+zFHFJsWAswzeO+mEQSMvTLUEgLbsL1rFUSiQeJFAui\nIdJqXCh5piwc0gcb/PpzFkLBYNBxrMA8RajQNGLOrlWdcCpAhUTiQSLFgiiI1IXdo+XalQNV\nJNap0SeCV2KT5TiGZxZpB8B03b2jWNzXTgWokEg8SKRYEAWR2PjaBS3NXsWGZowt02JXAMx1\n3BezSHeN85UEbJ/GuH5cBInEg0SKBVEQ6bqp7Vmu2rAEIEUd2HuYCvCh476YRRqeBTBTdYdd\n3l3vWIAKicSDRIoFURCJDaidUZNUq8Pf4iUM5cs1tC8Pww2O+2IWSfwKMy2UpgEKNWOxx/XI\nJlskJBIPEikWREGkgTEAefLfUjjJ7lGaJIfZGMS0oy2DT09PZ6MRzvsSJpKwFLNN3PZVXeVK\nNrRe4f5LkUg8SKRYEI1Ru3JcKLja+vLFmUUAbCj8+AAbr+vO06/RwiwXt0GEiSR2zjIUsNnD\nYylJJB4kUuFAQ+4AACAASURBVCxoKioq0p95UlpUdIGT2Eaktml6fS95KH0cZ/E9JVr0Uqfn\nqjDCRRI7tQIyT3h5vCuJxINEeu08q629LYfaq6qq9CeTPJ2v1PcpVYLwjiaSKNZtYE8/yVh1\n5RWfcizUb87NTMsqPtbnKRuJxINESlyGr+xaNW/Re2fZjXw9OwsLyvSRhWBLT6yeFW4LicSD\nRCJcQiLxIJEIl5BIPEikOBG0YaRnaLEq12JLJBIHEilOZIM1Ht6QGddyIyGReJBIcYJEcoBE\n4kEiJQ8kEg8SiXAJicSDRCJcQiLxIJEIlwQD7l+TySCReJBIbywkEg8SiXAJicSDRCJcQiLx\nIJEIl5BIPEgkwiUkEg8SiXBJ0N2otwaJxINEemMhkXiQSIRLSCQeJBLhEhKJB4lEuIRE4kEi\nES4hkXiQSIRLSCQeJBLhEhKJB4lEuIRE4kEiES4hkXiQSIRLSCQeJBLhEhKJB4lEuIRE4kEi\nJQGHi4s/jn02EokHiZQEfGB6VXmsspFIPEikJIBEcgGJRDhBIrmARCKcIJFcQCKNDrovHT54\noTM8NlR3quLEDTe3aFulFBpOf1751ePIF0s8OnvgZJ2+sXAj2i4c+eJSY8hrNgdIJB4kUlQI\nlLIXj0Pqhh5xL8A2JTa4O1N6oH16qdMj7a1SDu6fKj8Pf3bVsBJ1X3ofbP1iKTr1Q6nmHdKe\nm/9ETlS3RF6ceWzASzZHSCQeJFI0qB2v1srsJl2kR1laZU0/z81vlfLZbP3VEouVOikZ8WWa\nGj35qRhhxPBmPVtOwHU2F5BIPEikKHA3Hetj2tK1i1IA5mxVRWqazH7/F5fks7WptZz8VikD\n0kvN56z6seTYfPmcjxlx2w/pJXs+WMCiF+FZ37nCQsyeUVhYyBozoYzFj120ei77zHrqMpsb\nSCQeJNLIGZiFlXMrqzbtpQApikjDCzG2nFWOrk2sdbB/hZ5VSqGY6dOAQaGWvQJpu5QSjcic\nBiWsmyNcHqu9BcnQ2TmFkRPPsVPBerZXSwV32dxAIvEYLSIN396x/p2CWexKvLcvGAc+w6q5\nTw4KG9gvviRSNeidpY0Yvmqb3yrlDfws6JfjWmcA+J+z0H1W+jvKKMJhDF+UQroRfRkAM1qV\ncBEmuOAqmxXDoTD6mEjhkRyC3R4Sh0IvA52e0odavSXvCHhL3zXgKXlvoN9T+n5vR2cw0BU5\neKTiWqTQpz/8HZ9EHi79u3/cPeg2ZzwYxno+Q/2WnWmqFQUAE9RBuD6M3WVbgFXKVXiO91hN\ncB3LlCbzSEY0qkkx/JkU0o04gnHX1GzNWNYiV9ms6AyEw0SKiCTixrDdn8qtSNf+0qfCRPoT\nn++PjrrMGg9uY82s1JY2KiI1gdGdtzl11irlYCrAu1qckIu9JRZgRqzRorH3tF8K6EZgF2iZ\nXvJH2HMbdJPNiv6eMDqZSOGRHLraPSTu6ekOtHlK39PqLXlbwFv69i5PyTsCnZ7Sd3Z4St4d\naO+x7Ry4FGn3v/CFi+TzLYv+W7tflc+xmvZpS3WKSOx8rUmLbbl3r9Eiq4RVyrsYd05P8lNc\nZOfUzIiDWuycCCMGsIP2iZ7tEia/4yKbO6iPxCPx+0j7FId+68+NIvn+ydN+xJJ1AJP1pYAi\n0haAFPuzWiNWKY9jKff1xdO4eFuUjbisxUYaUY/rv9azNePiVy6yuYNE4pHwIn33fabN77x9\nSxAVkU7+DYv5nn3nPc4sBVigL4X8skioV5a7/FYpP8Wqb6hXdYoJzIjvtNhII65ABCddZHMH\nicQj4UWaw6xJkQaiFJFEYesvYfA/etqRGPIWQIlhcbos0gqlo++MVcqdAOmGk9dnaEK1KBuh\nXz+NNKI6UqSDLrK5g0TikegiPf9VdGaCPF6hiiSKe5hdNzztSezIA3jbsJgpi7QcYKG7/FYp\nPwbwG0732HjEadHZCHYKOHmKCRJJ4Q0XaQ0a86fK9RRdJPHvMbzc057EjmXKkJpMUOkjrQWY\n5i6/VcoD2lVTiVrl6pKTEVdx/dPILZBI4hsv0ng0ZruaXhepCsN/52lPYsd7ptOwJ/pgg39I\niw0Fg0G7cUarlF8qowsKbOyhQXQ2ogHX10VugUQS33iR/gMaox4xg0jtGP5jT3sSOw6bRtjU\naQo1pthKgFS7MTyrlEzHw3qSTRipXhDiGcEuPxkusQWbm5uHXWRzB4nEI9FF+m2f7/e09LpI\n4u/6fN/3tCexoxGr6Xp1QXhLEYkNPZdpaVYAzLXLb5VSwJ5WgdaEBScapihwjVhsmtSH8VMF\nN9lcQSLxSHSRfsPn+wMtvVmkX/e0J7FDKABIUXsmtdpcuyUY+1yJfYgtxYe2BVil/Mh4RXaP\nMoztbMQpZXhP4lkaQLmrbK4gkXgkukj/2uf7njoPzSBSL4b/zNOexJBzWE/nvJCCDydrIrFp\nBflyXerLU/o41lilbEKhxilZvvQDTJYOgr0RS+WogakAY5TJdu2z1ctHjtlcQSLxSHSR2PDc\nETW9LlIlhv+Tpz2JIcIarKgZe+oe3dw2BuZMB9gtxa7G2GlHWwafnsYo06Wm8PxWKT/BUOr2\nb4O9t0qZm5ekSBsjNqN03w13sRELNr8VSm92Bh9W4PkgfOAymxtIJB6JLtI6NObvlbN+g0j/\nDcMrPO1JLOku0K5/Tmiaqd5T0Z1nuC46i3McLFOG1hovq/5MTmhjxEE5kbSmwphtadBtNheQ\nSDwSXaRv2KXXTUp6TaRPWew1+1zxpn+jUnN/9FgcD3BAju0p0Wt0Oze/VcrQzlQ1LuOEEmdj\nRCDTYETNRK2sTUH32ZwhkXgkukhiGirzS+9LbZIm0vHfxODf2N6e8Tr4ZvOP0icVHg2KL7Fq\n1qixdRtYXc1YdcVxrrpVyifbWS/Hn/+J9uCfzqqqKv0PcKGqSplB9/jdrLRpC+Rumtj92YIU\nzDdz03eesjlCIvFIeJEa2Rwh39+dHtZEejhfuq/ismPW10Ir1uF6w3Kwxf4+EjNWKUOtHa/y\neyF0vnDzCDBvkEg8El4k8dPvSbdN/GBsEf7/D+/N/6/y7UnFnvYjlvRVVFTo83LYADSvPzR6\nIZF4JL5I4q5f9kUyP3Fu7AuNUW4Elyh0Pet7tEEi8RgFIok3/iJco9/+WeJ4JD1fYYL6fAXW\nINncBx+0IZG+Cg8SicdoEEkMbv9ro0a/v7zZ007EGnZf+LifBQRR+O59DObY/AWywRq3z5V7\n3ZBIPEaFSNh7/vqfx//7P/z+r/zgz//hnZPe9jgO7JeEGJs9jn1kfmuTikTiQiLxeCMeECmK\nVRmaFkVuL8uMOkgkHiRSVOg5ur5wcurUBbtuj5Yej3dIJB6JLlJvT09Pwj1a9c2EROKR6CL9\nwOfzrXdORsSeYMDbRV4SiUe8RfqXKNJiT5skYgSJxCPRRfq3KJLL58MRsYVE4pHoIk1Gkf4P\nT5skYgSJxCPRRWpg84PuetomERtIJB6JLpK4njVJfc7piFhDo3Y8El4k6RGR/3erp60SsYBE\n4pH4Iomf/67P91s/TuIrnaMEEolHwot05MiRD3+XTVX9vb/5x3EZRu542hNipJBIPBJeJIt7\nkRTOeNoTYqSQSDxIJMIlJBIPEolwCYnEI+FFKrTF7r4fIjaQSDwSXiQiUSCReJBIhEtIJB4k\nUoJxBgDU8POyuWOyLF4c9nogkXiQSAmGQaS6seze9YR5ECaJxINESjB0kYbmAInEgURSIZE0\nDhcXK8+Y1EW6g6ENLaGEmRpFIvFIeJGe2TLoaU8SG/3leW21tbVy6BRAeiJ9RxKJR8KL9GZc\nkLV6C+V+gNmvYVdsIZF4kEgJgY1Ic17DrthCIvEgkRICEolE4hIrkb6XmtC3Uby8fqyi5oEc\nHrh2tOJ8o+ktR0LD6c8rv3psGEZwFmnw8dXKozeaQqYkobpTFSdumO8Ajyxcpv9ihfa+87YL\nR7641BgKT8KHROKR8CL1mHnRcGTZn6FJ/5fHwxIn7gNki8IB+Z2SC5+J4tBe6YHgkH9PSzO4\nf6r8dOPZVbJeh7TnHT8RxcvyqF2tFicNf3dslouBqRV69Q/ulreTXtpiX7g0+odCXpyieVm3\nRE4y85invz2JxCPhRbJg+NM/8Pn+1WPnhPGHiRRarTqQ+byvUA37G5Qkz2brz89fLNU1Z5Hq\n9ffCQpE6kvcoS4tLP29buCLSWT8oIg1v1pPkeKlcJBKP0SiSKD79n3y+/zMRn2SMIs3YBPCj\n8m1FrKIuWQowvXRHMavFb8lNSWAaWzFn1Y8lDeaz07JzhYWTATIKCwtbNJHqCwunA4zBuFuY\nZzxGpuatXiHl2Spvqmkyi1xckp/OPmvtCpdFapRe7MxEEspYaOyi1XPZZ9bTyO9gB4nEY3SK\nJDb8qs+38tWzxwwUyQ+pR1kH5Zr8UvJdzJ/6SRh6yBIIxayGN2ACoZa952W7nE/vIykiicY+\nEnvr0gZWb4QLaE2K9G7m4YUYWc7qXtcm1rYItoWjSNm5kLLh+jesKrE3oU08x0776mdhcKn7\n670kEo9RKpI43+f7g6ER5I8R95k7p+XwDhbeI4dPYlA6/bqBgYJ+ObJ1Bmr3XApyRRIy8IRO\n6fEcxdXnWKAaA9uUrW7E8FXbwtkMCZjUKEf3YVkzlGcy9bFW84Lr70Yi8RitIp33+XzHR5A/\nRjCRFis/8uxFflnKiFovhitYYBWehmm9u+ugvn2WK1InRh1UsgQw/DkLFABMUIfr+tKw5bMt\nXBJJnUN+BMPX1CTNaZwX3vZ2htHORAqP5NDe5iFxZ2dHoNVT+s6At+StHtO3dXhK3h5o95be\n69Fp67Q9eRiJSAEU6d0R5I8RTKTDSrgDw5vVFXhutx8/BvF8T99tIVd1hStSEKP2qllCoRBr\nnJpAdkfmbSm7TeFMpGVq7AJDWBQ/Akizm4XUGQiHiRQRScSNYZu/1IhE6kSR8kaQP0bcN/z4\ns/qvNZrZskh31TMzmZ/iotTE8/tIWQBjw25LYmd2TdpSy717jbaFM5G+UOIGUgA+0ZNcwlWu\nL8fRqR2P0XpqdxlFmjyC/DGCiaSeXDGRLqkrFJGOY9x9PflpXLzNAnyRPmYnZyur2w0b2gKQ\nEn5B1abwOwa56zH8tZ6kGRe/cvvdSCQeo1WkUhQpdwT5YwQTSX2JbBAMNxMpIn2KcYb6Uqcm\n4Yv0coF82WfWxhr1cK7D/lf4xm0KZyI9UOKuQAQn3X43EonHKBXp+X+PIpW+ev5Y4STSToB0\nQ5/xGSaR5u3wRRKHKtKVap+6Um5cVlgME9gUzkR6rsRVR4p0MLwYO0gkHqNTpLr/hU24u+Sc\nMN44iYQnaX7DGVmTOljuIBL+WU+vUV3aygpYDrAwfOM2hTOR1DlE7HRv8hQTJJI1ySbSlgg2\nL/3hrzCP/o3tKMbrw0mkA4ZKLcoTgdgFIGeRkMFb+5enqG3IWoBp4Ru3Kdwo0lUMe5jMYIJE\n4pHwItnfRvGxpz2JD04ifamOLsiw4QFpDp4bkRjd+1Lx/C0oDTb49QvSoWAwKNgVbhSpwTDw\n4BUSicfoFWlawjzMwICTSE8M15mQTdjpka7jcEXq7+rSj2KFPDRXYxqhq8RyQnaFG0Vil5qO\n6kmCzc3Nrtt1EonHaBXpX65IwBM7Z5GETIAC7RcgOFEdMuCKhK1PmvZXbQRpWg8buC7TNrsC\nYK5t4UaRxMXKtDxR3exU179HJBKPhBfpby34fyaWPfK0G3HDSSQ2l0C/aLpHG33milRhPB87\npywsAUhRx+IeYjvzoW3hJpFOqeOEjGdpAOWuvxuJxCPhRRpdOIrUhHV+nHJr0pd+gMnyfDkU\naakcaSESu4q6QDmOXTlyH0malJAvV9W+PKU3ZF24SaSBqQBjlMl27ezmpe9cfzcSiQeJFFUc\nRRI/YReDtn8b7L1VykbglCH8zWjAd8NdQ5YiDaM8kHmwvrvvwQF2o5E06Vtgtw9OO9oy+PT0\ndAyV2BduEkmaywqlNzuDDyvY3YIfuP9uJBIPEimqOIsUWmu8GvozZfVBefGJ9ahdXaoxT558\no0R3niFuVrd94WaRpBNFjaXm5z1wIZF4kEhRxVkkMbRT0yLjhLo6kMkTSbyaqdf9UvUv0FOi\n+6BOw7MqPEwksUa/b32TB49IJC4JL9IPf/jDYxbRDzHe9XTL+NFZVVWlHqEQhrUKfK6qSuuN\nPNnOOif+/E869YyP381Km7bghSi2YC456lltrXZVqPPzouwUzDN38zeGrdVtYIJlrLpiGHmL\nLLwdS+w37mT3ZwvYhd2Zm9z3jxgkEo+EF8nn822xiH6B8e972pWEItTa4Xn4PhRoj/zbBlt6\nIsavXRQudL7w0hjJmyKROIxWkYIYv9jTrhAjhETiMVpFqmVzGzztCjFCSCQeCStSnzJFFYWZ\nGDFv9YPiP8X4BZ52hRghJBKPhBXpmf1sVZVNnnaFGCEkEo9RLNKvP/O0K8QIIZF4jF6RvvcT\nT3tCjBQSiUfCitSmTFBFZf6txbzVf8hz/2RDIiqQSDwSViQtkfWoHRF3SCQeJBLhkmDA2zVc\nEokHifTGQiLxSHiRjhw58tDTJokYQSLxSHiRiESBROIxWkUq/9u/TcDnQyY1JBKP0SrSWz7f\n340gO+EdEonHKBWp79/4fP/jq2cnXoGgt9FvEonL6xDp8ZYlPzIz/V/5fL7/ztOeECOFROKR\n+CJ1zbSZI/RXnvaEGCkkEo+EF6nvP9Pk78SAROKR8CIts9HolxZ6+6LESCGReCS6SO2/yqz5\n8xkLxrA3UMwvLCzM/y+/i6GFHg8LMWJIJB6JLtJO5tEa9nyPoxg4JcV1/Bef7wdP+PmIqEMi\n8Uh0kSajPn45+L/5fLOVUsf6fP/7kH0mIhaQSDwSXaS/QpGUV4Mv9fn+Wont+hOf70NPO0KM\nGBKJR6KL9HsoklLEJz7fr6lvdtyOJ3fe9oQYKSQSj0QX6dd8vt9UgjfRqW+VcPDXfb7PPe1J\nIvO8bO6YrFd9kV7cIJF4JLpIv+zz/ZES7EGRjqjx/6/PN8nTniQwdWPB+JjwKDJ4uSx3Uurk\ngh2NpuiGrfMyxudsvuOtMBKJR6KL9AOf71+qz+T9A59vtRo/2+f7S097krgMzYEYifT1DO1x\n+ev0v0PPBu15/J6qLonEI9FF+p+xGVKfRP+ffT5Q4xf4fP+Dpz1JXNjbIja0hKL/Stxjxhe4\nzFTrXV++4Q0xfR6KI5F4JLpI6SjSTiU83ef7fbW6jfP5fsXTniQupwDSB2NQ7s/9qMrcQ7fv\nV0tNUL5yvYC9DGZqxS9ufjYF1PeTuYNE4pHoIv0ERfoL5SutwbDyKhfhL3y+H3jak8RlP8Ds\nGBQrsJZnmzzMeYed48n9y1sYmi+98qUjF4O37QsIh0Tikegi3f8ltOfvW6XwZRaUo49j8G88\n7UniYnihWDS5h5qsVhvwu9g6zZAWlgKkNcmRj1O1N9e6gUTikegiiWOkpxP/f1+ywthFpUL2\nG/vzP2EhT3uSuMRIpC0okv7YmHJceoCfbWB4b+wmXHBfHUkkHgkvUt2vSZO9V7FwHgv9xY+W\ngzST9aanPUlcIkTqv1hRrQQHb1UfqLzQEHYQQ3WnKk7ccLjxuxhglj6AUYPOsGfTHsXPejXy\nLi5YvQ7RGhKJR8KLJH6qi9T624b7KMZ42pG4sBHArxwfYRJWUmVuk7gNw4+ss9RqI2iXpfE7\nVOriFNWsx2Xp8jr/yvt6luBu+Y2y6aUt1mXKzANYpS99o+zOOoDx2uv8QuMA1rr+diQSj8QX\nSTzyh6pI4qf/QvPozxLwPopzWFuvysHH0nUaJX4+QJbN6HakSGfZYJskUrXhZeb+M2qOR1la\nZPp5zs5kAGzVl65h8huipNdyPXYZQK7rb0ci8RgFIomDx9bP3CcHD/+O4tHfJ+I7XbrRgY/k\n4ElW0acKWrTd7bz1hYXTAcYUFhbekkVqlOxhIt2VXMlfvTKXqaWOEDRNxoXUxSX5rLFKrbXf\nmd7eXsOg+k5MjYcshKWX67HYhI5x/S5bEonHaBDJSM9PUv7qj//XWaeif/kyGiwEeEsOlUlN\nxlMpfAVDV+wz6X0kFCk7F1I2XP8G/+jCAjZ8LQ1UN6/B4HYpyTBuAspZletiQwU5Lo9DJ57E\nzcW0LZhnhx69HRdd1y8SicdoEymx2YcVU64O2fhbD1AlhT/CpqPfPpNJJIBJyrw4Nr62XvFk\n6C0AeZCymtmlZNyon0k6MLgYk57BwEP83G/aNDy2yfNyIIweFCk8jkdvp5fUA8FAh6f0A63e\nkrcHvKXv6POUvDvQ4yl9T5en5P14dGx/NJNPpAZlbEwMAEzGfn2ZFJtn6pdEECaSOgu8DsP3\n1DS7AWZKgQKACepwXV8awC43u/ViCZYlXTdgF5eO6ivYJKJvbDJ1BsJBkSLiiPhhexKefCIN\nZwJI7844A1ByXBli6MEuzkFOJrNIy9Tob3fv3qPefiVW4kkf+2wCoztvAxS72KmTE1ivS2op\nb2LorL7qnEHccAaDYXSjSOFxPHo6vaQO4m+up/TBVm/J2wPe0nf0ekqOLZKn9B6PTh8enTeo\nRRLXY1eEfX4AcIgN3D3H8FXO+RPDLNIXlmk2KyKxM7smLbbl3r1Gy+RG6t5i7VyuPFTORDqn\nr0Pf4WvHEhSoj8RjtPSRhm/vWP9OwayPMZjIj+E6jTWzHT9zAO4LUwBqMLxDnZ1jg1kkqwai\n73CqItIWgJSQRQpbOtZJg+e7lINWb7i6JYZdnnWCROIxKkQKffpDZdg7D5f+3T/ujsVk6ajQ\ngTXzK/wjAIwPiaUA72NcPsBmXh6zSA9M67rra/aUzGHD37JIqEWWl/25KF27LdRkeYBLn5k2\nDa7fPkUi8RgNIl37S+0yLBPpT3y+PzrqmOk1gdb8RBrwLhbF41Lt7/U7jK2ZRXqurxg4uMiv\nX5KVRFoBsMj9zvRIg/CzzuvtIRv+3qknwMYSuNMjjJBIPEaBSLv16QyqSD7fssS8kCTuxYor\nVVD83X+C1fQFm1OQxp0WZxZJr9d31Ptbxxbs26aItBxgoet9aZvLrugeND63bNh8QbYcINX1\nmSKJxCPxRdqnOPRbf24UyfdPnvYjbsguLJBu9BGmAnzJ5hSs4GaxEelRBpu6sHTvtef4m3FI\nEWktwDS3u9LHbmFf0myOzDEN9BUD5LgtjkTikvAiffd9ps3vvH1LEBWRTv4Ni/meu0uR8SaE\n1b8mmAJjWDcOO0n/zK788Aa/bUVajQsl6kSoQ/pgg19vYULBoP2YKBtBhPLw9gY7WZlaFiGT\nJq3akmwizWHWpEh39ikiicJWdrfff/S0I3EDa+rGm8oNcycAZvf5+YPfdiJ1Yb7l2iU4VSR2\nN4Q+EbySd2bGLg6XRWh2wji8wAboT7r4TjIkEo9EF+k5u/VoglyhVJFEcQ+z64anPYkXpwCy\nfgqwl4XZ5dMqVIDfn7MWSZskIbFXEakZ1OkSjBXKVStL3gWYFHnwX2ABu9WFPbjQ6vSFNEgk\nHokuEntOw58qE9V0kcS/xzBv2s3rI4CVcyaANC2bdZJmOgx+24l03dT2LFdEEpcApKgDew9T\nAWwf3Dw4TplkEcYygAmdcrAr06n7ZoJE4pHoIo1HY7ar6XWRqnwJ+zLmXOkKqPycq/XSqJtD\nb85apHplmqlEtTr8LV7CUL5cQ/vyMNxgV2ojrizeYUISkE3hWyWdDw6xXhg9/MSOJBPpP6Ax\n6hEziNSO4T/2tCdxYxdzp0AOS3cl8Qe/7UQaGAOQJ/8thZPsHiX5wbICq/3TjrYMPj09HXiP\n07oEEUhPVhXYVIeldcG+ny8C/dZDN5BIPBJdpN/2+X5PS6+LJP6uz/d9T3sSN9hPvnp731MW\nftshg82oHXtcScHV1pcvzmCNZ0PhxwfYeF13nkGNWfZH96CNSGL/PD1mvoPjJkgkHoku0m/4\nfH+gpTeL9Oue9iRuDGHnBC7JYWEahisdMtiI1DZNr/AlD6UPaZZcT4kWvbTdvtStdiKJvVoB\npV4etEoicUl0kf61z/c99WfTIFIve2qDpz2JH9X79u1Tv/UZDDv9AZ/V1io9lfaqqir9DsCn\n85X6PqVKEN7RRMI2bwObQZex6sorzu4Qbm/MGT8+p/yut/wkEo9EF+nvDa+gMIhUieH/5GlP\nRiHDV3atmrfovbPsh6RnZ2FBmT6yEGzpifscKRKJR6KLtI496ESpMwaR/huGPYzcElGAROKR\n6CJ9wy69Ko/g0UWSnnV3zdOeECOFROKR6CKJaajML70vtUmaSMd/kz362/VzpBICu3uIR3qG\nFqtyLbZEInFIeJEapccT/93pYU2kh/Ol+ypi8Ya7GJIdOYom4fp2oDiXGwmJxCPhRRI//Z50\n28QPxhbh///w3vz/Kt+eVOxpP14/JJIDJBKPaNzYt+uXfZHMT9Ab+5IYEonHKBBJvPEX4Rr9\n9s/Io7hDIvEYDSKJwe1/bdTo95c3O+chok0w4GVCEYnE5zU9jkv4+p/H//s//P6v/ODP/+Gd\nk972mIgSJBKPUSIS8fohkXiQSIRLSCQeJBLhEhKJB4lEuIRE4pGYIv0G0mS7lngtBL2NfpNI\nXOIjEhvkfuJpQ0TMIZF4jAqROgoRT9slog+JxGNUiPSELXvaLhF9SCQeJBLhEhKJB4lEuIRE\n4kEiES4hkXiQSIRLSCQeJBLhEhKJB4lEuIRE4kEieecMAHBW34/Jjd6e2Ks8fdxhT71BIvEg\nkbxDIrmCROJBIpFILiGReJBI0RXpcHHxx1HYpTDcieRx2yQSDxLJO221tbWc1Z5E+sD0lvFo\noYrE31OP2yaReJBIUSeBRIrqtkkkHiRS1CGRZEgkHiSSIySSDInEI8lFGrxVfaDyQoN5pwcf\nX608eqMpZJvr0dkDJ+s61SVFJOF+TcXxm5x36kmYK7PVlhy3rhM4V/nFOeawpUgRBZFIPJJG\npDP1045fEQAAIABJREFUGmfYcn0YMXhI5OOydPnB2f6V97XIjs3j5MipFUodvKyOhd2X3jVe\nv1hanfqhUhFkkWrnyCWVc/5+h7QndT+x3pJ1nA33i+SURY2aSNqeRhYUtm0XkEg8ElckJ7I9\n7YgbqlP1Z9D7zyiR9RP1yKJBKcos0pdp6urJT9VYaPlEyzThoe0GzZXZYkuWcTYc8KsJU89E\nihRREInkAIn0qtxltSo9f/XKXFYl0+SHrwTGs5qZt3pFFlu7VYoziXTbD+klez5YwFYvEpRY\n2A2Q8e6eTbkstsD2OeXnCgsnY8LCwsIW6y1ZxdlQKUkxr7ggBXMsCxcpsiDTtl1BIvEgkVQE\n5sI2qafTvAaD26XY9zG0gf3JhAt42pcirTaKlDkNSlikcHksKGMM96UqXc7eGC5cHINB3l9Q\n76dYbckqzpqHrDUteSayczhp+2aRLAuiPhIPEukVacO6tl5pPIbeApCetiJk4JmQ8mrAo5jg\nHAsYRQJ4R+m7HMbwRS32PaWkUxi+wtmqVpmttmS5dWt+zH4FlG1WRIhkXRCJxCMpRCp2wSFP\nO+JMHVawe+oCnprNZJ+dGHlQiQtg+HMWMIvUqKzuw/BnamzaCyW2Fxd4e6pVZqstWW7dkhY8\nG52hjkYI88JFsi6IL1JXazgTJ0ZEcQl4TO41vbfknnfHa/GxLR+Lt33Xa2JdGvp29+492sBY\npTQeh7/CWOv2KnFCKBSSvotJpDVaAdj92K/GbtJip7oUyWpLllu3hDWHx7WlmnCRrAvii9TT\nEUbbxInhUVza27wlD7R6St8R8Ja81Wv6dk/J2wIev673o2Pb104skUxsVkRidoytC1tpEumg\nFj3HIFKVFpvtUiTLLVnFWbIeYJw+qDeUEd5HsiyITu14JMWp3Wum73CqKtLH7OxtZbXpuqpJ\nJP2d0EaR9ErrWiSrLVnFWYJ9uvmGxbxwkSwLIpF4kEgjobu+Zk/JHOmKjCzSywXyxZZZG2u0\nb2IS6Tstr1Ek/Wqxa5GstmQVZwm2OCWGxTXhIlkWRCLxIJFemYGDi7SLmqpI4lCFMtsBUlcq\n7YxJJP1yplEk/dqMa5GstmQZZwWey31kWNwWcUHWqiASiQeJ9KrcmaFUtbEF+7ZpIuERPb1G\nrYRbpdGIGIlksSWbuEimApQbFn8SOUXIoiASiQeJ9Io8ymA/10v3XnsuSBNojNepBm/tX54C\n6tBCzESK2JJ9XBg5AMsMiyssRIosiETiQSK9IqtBmRrACBOJ0b0vFSCdvWsrliKZt8SPM7AS\nYIZhcZa1SGEFkUg8SKRXowu7R8u1CzWqSP1dXfoXYDMG2KzwmIhktSXLrVuyD1c+15ae+cNE\nsi6IROJBIr0aDVi9LmhLexWRtgCkaQe0UUkSE5GstmS5dUvYtIxdxkLNIlkXRCLxIJFejeum\nH/zlikjs11sbLTunLMREJKstWW7dEmEm9u/UyUrPx4SLZF0QicSDRHo16rF6nVEXqtXhbxa7\nQPkKXTmx6SMt1bYfviXLrVvDpqJO/VYKPpNuKTSJZF2Qtm13kEg8SCSFAfwZz5OPnXCS3ZMw\niQWHsdZB5sH67r4HB9itPNtYZDRF2gww7rvhriHLLVlu3ZrhQlyfsvHivRs7xgEsChPJuiBt\n2+4gkXiQSCrl7Gf8auvLF2ewGrKh8OMDWMXqDHfNomn9LGU0RTooF/zEektWcTa0ztYTLvgu\nfNTOsiB9264gkXiQSCpt0/SaVvJQ+mDzqa9m6tGl8pd3EKmpqKhIn9RWWlRkO0SABDK1ymyx\nJcs4u/1fpSZc2/eiqGiTaU8tCzJs2w0kEg8SSePpfKWiTakShHdUkcTOz4uyUwD8czd/oyRs\nqaqSZ3d3YkA/Hheqqr6LKNSRx+9mpU1b8MJ6S9Zxdtz+ICcjfda7tfp0e21PrQsybNsFJBIP\nEkln+MquVfMWvXeW9cR7dhYWlDWoa0KBdm+H9ZWx2lLUtj6ygkgkHiQS4RISiQeJRLiEROJB\nIsWDoA22Nw/HvKBXgETiQSLFg2ywxvP7yKJW0CtAIvEgkeIBieQMicSDREoeSCQeJBLhEhKJ\nB4lEuIRE4kEiES4JBuwnn1tBIvEgkd5YSCQeJBLhEhKJB4lEuIRE4kEiES4hkXiQSIRLaNSO\nB4lEuIRE4kEiES4hkXiQSIRLSCQeJBLhEhKJB4lEuIRE4kEiES4hkXiQSIRLSCQeJBLhEhKJ\nB4lEuIRE4kEiJQDPy+aOyeK9IjYhIJF4kEivn7qx7LELl2NV/LaM3LCY4drS3HHj52196Kkc\nEokHifTaGZJewxIzkUKZ4W/xbMxRn5nyods3UTBIJB4k0mvnDtboDS2hWD2a7mr463DPp+lP\nHyoZtsllAYnEg0R67ZwCSB+MWeldWWEi3RsD4F9ztv7cT/zAe1F6BCQSDxLptbMfYHasyhbq\n2AvIjCINZQOMvSoF76YDjHd/jxGJxINEeu2gSHNiUvCtw5typRM4o0jHcblGCR/D8FXXxZFI\nPEik107MRNLeLWYQScAGaaHaHXuZAbDJdXEkEg8S6bUTIVL/xYpqJTh4q/pA5YWGsIMYqjtV\nceKG40mZlUjf4uJ5benE1q37Xe8nicSDRIoiG7EbrxwfYZL6xj9kG4YfWWep1QbQLkvjd6jU\nxSmqWY/L0uV1/pX39SzB3bIh6aUOT/z+sopRaBJpH0Aq71W0HEgkHiRSFDmn9zkes6peqsTP\nB8iyGd2OFOksG0yTRKo2vEHZf0bN8ShLi0w/b12oiTKTSCvwzO4VvhmDROJBIkWRbnTgIzl4\nklX0qYIWbdcVqS8snA4wprCw8JYsUqNkDxPpruRK/uqVuUyttCY5Q9NkXEhdXJLPGqvUWued\nMouUpevtFRKJB4kUTRYCvCWHyqQm46kUvoKhK/aZ9D4SipSdCykbrn+Df3RhAWbb1snim9dg\ncLuUZBg3AeWsynVtwlCO82Vck0hBVqY4fGnd7PQJ8z685+nLkUg8SKRogj0QkKtDNjYzAPIr\nxT/i90tMIgFMapQX2jC8XvFk6C2AQilULZkgs9HV6LVJpCbMUtG8SD03XMv560RAIvEgkaJJ\nA1bOCywQAJi8DqBMis0DWM7JFCaSOgu8DsNak7EbYKYUKACYoA7X9aUB7HLcJ5NI32GhP5uq\nd73mtNrmC/aG0TlxYngUl652T8l7At7S97Z6S94W8Ja+vdtT8s5Al6f0XR2ekrOjY3v2kXwi\nDWcCbGGBMwAlx5Uhhh4/fyaOWaRlavS3u3fvCakLlYoNrEXZpeV8G6DYcZ9MIrF+F3bCJu84\nc/tEOet6LQnZ5esMhDNxYkQUEUds50Umn0jieoC57PMDgENs4O65KM0ahcecPGaRvrBMs1mx\ngZ3ZNWmxLffuNTrukkmkr6V2aJ18EnWXjf9V2+ULDYXRN3FieBSX/i5PyQcDnZ7SD7V5S94R\n8Ja+K+gpeW+gz1P6vm5PyQcDXfYz9ZNQpNNYNdvxMwfgvjBFnoyzA2AGb0zALJLV/X19h1MV\nG7YApNi2IdaYRPoF82ixujdse3NczzqnPhIP6iNFlQ6sml/hHwFgfEgsBXgf4/IBNvPymEV6\nYFrXXV+zp2SOX5uegP2uLI+7ZBKJdeJS9GvDJbjY7LYgEokHiRRd0JqfSAPexdL8UKzCvX6H\nsTWzSM/1FQMHF/n1cQHJhhUAizzukUmkh1iQ4X7ZGlz80m1BJBIPEim67AWYJZ3NfSaKT7Ca\nvhCvAaRxp8WZRdKn/dyZoSg0tmDfNsWG5d4nJphEagfTBdl6206ZBSQSDxIpusguLAC4LYrC\nVPZ7vxNgBTeLjUiPMtgI29K9155jL+aQYsNagGke98gkkpChX4ZCWnATrmetkkg8SKToEsKa\nWhNMgTHsnlfsJP0zu/LDvw3VRqTVuFDyTFk4pA82+PXRm1AwGHQcKzBPESo0jZiza1UnnApQ\nIZF4kEhRZh3AxpsAS1n4BMDsPj9/8NtOpC7Mt1y7cqCKxDo1+kTwSmyyHMfwzCLhSed03b2j\nWNzXTgWokEg8SKQocwog66cAe1mYXT6twnrMbzWsRdImSUjsVWxoBnW6BGOFctWKi1mku8Z5\nfwK2T2NcPy6CROJBIkWZANbUmQDStGzWSZrpMPhtJ9J1U9uzXLVhCUCKOrD3MBXgQ8cdMos0\nPAt3SXWHXd5d71iAConEg0SKNtJjEvx9Uni9NOrmMLHUWiQ2oHZGTVKtDn+LlzCUL9fQvjwM\nNzjuj1kk8SvMtPAFCwk1Y3FHbW44tIBE4kEiRZtdzJ0COSzdlcQf/LYTaWAMQJ78txROsnuU\nJslhNgYx7WjL4NPT09lohPP+hIkkLMVsE7d9VVe5ku1chfsvRiLxIJGiDRsJU2/ve8rCbztk\nsBm1K2c+Xm19+eLMIgA2FH58gI3Xdefp12hhlovbIMJEEjtnGQrY7OGxlCQSDxIp2gyNwwp6\nSQ4L0zBc6ZDBRqS2aXp9L3kofUjPgOgp0aKXtrvYn3CRxE6tgMwTXh7vSiLxIJGiTvW+ffvU\nb30Gw05/wGe1tbflUHtVVZV+B+DT+Up9n1IlCO9oImGbt4E9/SRj1ZVXfMqxUL85NzMtq/hY\nn6dsJBIPEilxGb6ya9W8Re+dZX2snp2FBWX6yEKwpSdWzwq3hUTiQSIRLiGReJBIhEtIJB4k\nUpwI2jDSM7RYlWuxJRKJA4kUJ7LBGoenpb62ciMhkXiQSHGCRHKAROJBIiUPJBIPEolwCYnE\ng0QiXEIi8SCRCJcEA+5fk8kgkXiQSG8sJBIPEolwCYnEg0QiXEIi8SCRCJeQSDxIJMIlJBIP\nEolwSXAiiWQPiUS4hETiQSIRLiGReJBIhEtIJB4kEuESEokHiUS4hETiQSIRLiGReJBIhEtI\nJB4kEuESEokHiUS4hETiQSJ54QwAvO59eG2QSDxIJC+MNpEGL5flTkqdXLCj0RTdsHVexvic\nzXe8FUYi8SCRvDDKRPp6hvZQoXX636FngxpZ6qnqkkg8SCQvjC6RjhkfzzVTrXd9+XpknpfH\n6JNIPEgkL7TV1ta+7n1wzc/9qMrcQ7fvV0tNUL7yNnT2UpepFb+4+dkUcPWiMg0SiQeJlKwI\nrOXZJr/0/A47xzsiBW9haH4nC3Wwl3Tedl8gicSDREpW7qEmq9UngN/F1mmGtLAUIK1Jjnyc\nCrDUfYEkEg8SKVnZgiI91JbYizQf4Gcbfn6gRm7CBffVkUTiQSJF8vL6sYqaB3J44NrRivON\nw3ZJH509cLKu07nIwcdXK4/eaAqZIkN1pypO3DDXTqHh9OeVXz2OeJdE/8WKajXcduHIF5ca\nQ+FJwigGmKUXU4POXMDPo/hZr0bexYVjzjuvQCLxSFaRNgL4lV0VJoH21khxG4Yf2WW6z163\nKhzIlEa0Fj4TxaG946Rw/j05wWV11E5KKdYvltamfuhQBTo2y8VgJ1+v/sHd8nbSS/Xn3Q/u\nnyonnF2luntHesPsxSnai2brlijDcMf4f7l5AKv0pW+UY7AOYLz2sxDC3VrL33UDJBKPZBXp\nHFacq3LwsXTJRImfD5Bl++ogpkdotTo2nPm8r1AN++W3TppF+jJNXT35KW9f6ifqA85Fg0rk\noywtLv28Evdstp5wsVINJZHOsgE4SaThzXqSHG7VyADYqi9dw/Q3REmv5XrsMoBcXhkmSCQe\nySpSN1a9j+TgSaktELToTbaZUI8Z2G/4Ufm2IpZnCXbMp5fuKGa1+C2pKTGJdNsP6SV7PljA\nki7ivNcrMJ61WnmrV0jqKJW7aTKLXFySn84+5TH1gPQe8zmrfiwlnC9XWyZSYyooIgllLDR2\n0eq57DOL529vb++gvrQTk2MTG8KSyvVYbLfH2J62hkMi8UhWkcSFWPnlkFT3QK5zVzB0xTYP\n6uGH1KNMimtS3YVdzJ/6SWq/3ShS5jQoYb0j4fJY4L6O6H1cvYH91YULaE2K1KMaxt2DclZz\nuliPP4dtUihm+jRgUKhlbz3aLuVGkbJzIWXD9W9YRTiF8RPPsbpfPwuDS92+l68TT+LmYuIW\nzLRDj96Oi67rF4nEI2lF2od1RP7LYKUcA1AlhT/Cn/9+2zz3mTun5fAOFt4jh1mbJp1+GUUC\neEfp8BzG8EXbQgU8xypSfvZZV/8cC1RjYJuSYKNyFnoDPwuUnWudgU4/Z6E7bEuTlLlyfVjW\njFYlzFrNCy6OBDLIOnNnMPAQP/fr8ftx8bFNnuFQGH0T+8KjuAS7PSV/Gej0lD7U6i15R8Bb\n+q4BT8l7A/2e0vd7OzqDgS770aWYitSgVrMAdmGwi10mxeaZugjhMD0WKz/ybEQrS/kJ7sVw\nBQuYRVJngvZh+DPbQjtx7UEljPsCn7NAAcAE9fe9D7tau/BzFUqu1errmPJjFpBEqlNij2D4\nmpqkGfMt4hwBnRdsfKKQ/SHYxaWj+go2iegbu/0OhDMxIoaIJ7Yn4TEVaTgTYAsLnAEoOa4M\nMfT49UptAdPjsBLuwPBmdcUk5XfcJNIaLV+W6Wc+jCAm3auEBfxtYYejCWR3ZN4GKMZWA08m\n39XihFxldIGJtEyNXWAIS81rmqEbZMfwyQmshyU1zzcxdFZfdc4gaTj9PWF0TuwMj+LS1e4p\neXegzVP6nlZvydsC3tK3d3lK3hHwdnQ6Ozwl7w6099iex8f2OtJ67BWwzw8ADrGBO3aidJVz\nKiPKeqj1itV/dcycnR1GiqQbOYcnEtNsbFhtZWd2TdpSy717jXITeE5P8lNcZKfRTKQvlLiB\nFIBP9CSXcJXzrRB1b7E2LVfuxN00b4RNwv3asQQF6iPxSNo+kngaK0k7fuYA3BemANSIUsdn\nBqd/ft/gGRPpkrrCUqTLWj6+SB+ziryyut0QtQUgJfyUF1tNuG/efTYR7o5B7npzvW/Gxa/s\ntyvRsY5t3b/rpV7CcX2t6fKsEyQSj+QVqUOuZt0A40NiKcD7GJdvOF2zgOnxRAkHjapYivSd\nlo8v0ktpgBxg1sYa9WBg7c4KT/apNjoiUadsn4n0QIm7AhGc5Hwd5KJ00bdQk+WBuTu3H4zT\niBwgkXgkr0jMmp9Ila9Y+rnPFsVev3aV1hJvIj3R8vFFEocq0pVqn7pSblxWWAwT7ARIN7SW\nzzA5mxR0RzkrZVRHisTp8WGXsEwS+LxeLBv+3qknYEOTnIF7MyQSjyQWaS/WIamu4E/wE6wx\nL9jl/TReZYiRSPhHOb1GdWkrO6VbDrAwPA2eAfoNp3tsPIKNxN8x1HV2ujd5igmeSG3sqm36\nwSFD1LD5gmw5uu00Z0+DROKRxCLJVXCB1NUQpgJ8yX70V/ByxEwkZPDW/uUpahuyFmBaeIID\n5tahFuTW0ygSGyvhTkYy0Ye7BUuazZE5UgOtUgyQ47o8EolHEosUygCoCabAGDZEjJ2kf2ZX\nb7inQrEUidG9DxuE9KA02ODXG4pQMBgUxC/BdJsdG3tgE/yMIjWA/Wh1JOsxdXl4e4O9s0zt\nTE/IpEmrtpBIGlhpNt5U7l07ATC7z88d/I6RSP1dXfoxqJCH5mpMI3SV0gkWO/s8rGfbhJHs\nF8AoErvUZLicGmxubrafKcesK4sYojxhHF547DxaYYBE4pHMIp0CyPqpcjWUdTmqALK5k9Ni\nIhK2Pmna36QRpPkWbOC6TEuxQrrixZqHAm33ghOV8QijSOJiZVqezAfaXFwr3gWYFHnwX2Bx\nu9WFPbjQaltAOCQSj2QWiU3ImQkgTa1mnaSZ/MHvGIlUYTwfU6cSLAFIUcfiHmI786EoTVTQ\nL5buURsLk0inlKE8iWdppoGDMAbHKTM7wlgGMEG5E7Er06HPaIZE4pHMIons6R7glx85tV4a\nM+MNfsdIJHYNdIFyFLpy5D6SNCkhX65ofXlKb6gJhRon3/ckfolnoZOlemsSaQB/DsYok+3a\n2c1L+rWscFjbV7zDhGQuuz61Suo5Da0GeviJPSSSzi7mToEclu5K4g5+x0ikYZQHMg/Wd/c9\nOMBuNJImfQusEk872jL49PR0UJ+K9Qm70rT922DvrVK2s/K8CpNI0lxWKL3ZGXxYwe4W/MBi\newqXIq85SdOJBDbVYWldsO/ni0C/39ENJBKPpBaJ/fqqt/c9ZeG3+eljM2pXl2qszXnyjRLd\neYa4WfJBCq01JvyZnNssknSiqLGUU7MP2ogk9s/TY+Z7UYNE4pHUIg2N037YRYHdflrJTx+j\n4e+rmXrVLVWPX0+J7oM6DS+0U3Mu44QSFyaSWKPft76JV7G32okk9mpbLvXyoFUSiUtSiyRW\n79u3T92BMxh2OJadVVVV6vcLYVirwOeqqqTeSAtGRqQUxQvKattyPy/KTsHe2tzNxpt/6jYw\nwTJWXTGMvD3Zzno+/vxPtCcTteOWTLcidn+2gF3YnbmJu0kewu2NOePH55TfdXt/rQyJxCO5\nRUogQoH2yL9MsCXyJpNQa4fTYxSEzhfe6nRUIJF4kEiES0gkHiQS4RISicebKFLQBm9dhviU\nmkCbJZF4vIkiZUcOaEm4vjMnjqUm0GZJJB4kEonkEhKJx5soEvFKkEg8SCTCJSQSDxKJcAmJ\nxINEIlwSDJBI9pBIhEtIJB4kEuESEokHiUS4hETiQSIRLiGReJBIhEto1I4HiUS4hETiQSIR\nLiGReJBIhEtIJB4kEuESEokHiUS4hETiQSIRLiGReJBIhEtIJB4kEuESEokHiUS4hETikZwi\n3Y/9Ld5vHiQSDxIp6dmWkRsWM1xbmjtu/Lytrl9oLkEi8SCRkp1QJnuhu5HGHPUJKR8OWeex\nhETiQSIlO1chTKTzafqzhkqcno5sgETiQSIlOV1ZYSLdGwPgX3O2/txP/ODwcmozJBIPEimp\nEeoKwSzSUDbAWPnFhXfTAca7l4NE4kEiJS+3Dm+SXv5pEuk4Ltco4WPg9DJQIyQSDxIpedHe\nb2YQScAGaaH6XPCXGQCbXBdHIvFIapGE+zUVx2+2O6dHXl4/VlHzQA4PXDtacb7R1A8fvFV9\noPJCA+fbcBl8fLXy6I2mkCkyVHeq4sQNc+0UGk5/XvnV44hH4PdfrNBeZ9524cgXlxpD4Uki\nsBLpW1w8ry2d2LqV96pBMyQSj2QWqXaOVI385dxDeJ9VNOGAXOsWPsNexN5xUjj/npbmcVm6\nXCX9K+/LMZsx3KCursE11REF63RslkuEqRV69Q/uljeZXqq3nYP7p8oJZ1epGt8BmCOKF6dI\nH4y6JXKSmcec/nJfVjEKTSLtA0jtt8/Cg0TikcQifaIN8k7gXXlkIoVWq0kzn/cVqmHNlGrD\n65T9Z6SofjxFmqdchWmfAPBPnNeo1OuvfYWiQSXyUZYWl642Ec9m6wkXK9VQEuksG2CTRBre\nrCfJcVU1ykwircAfCze5LCCReCSvSLsBMt7dI3e3CzjVHNPO2ATwo/JtRSzpkqUA00t3FLOq\n+5bcftyVanv+6pW5LDKtSYq8CdqLx9eif5zzx8B4TJqat3qFpM5WObJpMotcXJLPmrrUWjkh\ne180zFn1Yymh8sZxJlKjJDITSShjobGLVs9ln1lPXRwLs0hYdKmLTFaQSDySVySAcvbSbuHi\nGAxyDiKm9UPqUabaNbnl2cX8qZ+EIaklExZgaJv0duTmNRjcLufDpiFVWn8JjN2OSN7H9RvY\nX124gNakSAUNL2T7x2pO1ybWtrCtC8VMnwYMCrXZ2nZQpOxcSNlw/Rv2HU5h/MRz7LSvfhYG\nl7p4nZhJJPau9m3i8KV1s9MnzPvwHidbJCQSjyQW6T2llrHKd8Uh7Wk5vIOF98jhk6ofbRhY\nr5Q19BZAoRxkJ3cFqFzPVP6PvJCBJ3RKj+coFnWOBaqlCi2zEeRB6Bus6VT6L60zUO/nLHSH\n7dKkRjm6D8ua0aqEWQN6wfFQmEVqwjwVzYvUk8O1XgZPSCQeyStS2gtloRcXDvHTLlY8YSdx\nWUE9WwUL1GFA++3GE8aZSvAXGH9AanCm8upjJ+jzBwIY/pwFCrDjplbLvjRsBPFzFTZxj9Vc\n1zHlxywgiVSnxB7B8DU1STPmW8TZsIJJpO/YCelUvZs1p9U2X29nGO0T28OjuLS3eUreEWj1\nlL4z4C15q8f0bR2ekrcHYnx02jptTz9iK5J+gWSqo0iHlXAHhjerK/DcThob/nb37j3aYFul\noVriyV36058b67YV7GxqrxIWQqEQa5xYw7BLS/E2QDH+gON55btanJCrjC4wkZapsQsMYVH8\nCH8tBkUnTCKxXwrczuQdZ26fKGc9viW2o+idgXAmRsQQ8cR2XmRsRarSlrIdRVJ/8VmlP27I\nFnmRZbOhWgYxxRL8V87fGezfj60zR7EzuyZtqeXevUa5jp/Tk/wUF1nrz0T6QokbSAH4RE/C\nOmd3+NsWw0T6WmqH1sknUXfZoAZv2N4MndrxSN5TO73uOouknlExkS4ZsoWL1Hc41VgtfyFV\nyxl9/J35mCVaWW0c19sCkBLeFLC5O/f1xdO4eFuURVK/Sj2Gv9aTNOPiV/xti2EiSXusnshK\nZc9x/fpzEolH8orUrC05i/RECTORLhuy6SJ119fsKZnDToaMg8lb2PIvHHbm5QK5PzJrY416\nMNZhVyw82aeYxFCV6pRdYZX9gRJ3BSI46bDxMJEaMEvKI22xxHScHCCReCSvSPp8gRGLNHBw\nkV+vuwaR2jB6luNcnaEKZV4EpK6UG5cVFsMEO7HHZWgdnimnXUyk50pcdaRIzrdBmER6iFkM\n98uyKRlfOpagQCLxIJGcRbozQ6m1Ywv2bTOJVA6GHgyH7tNrVJe2Mu+WW8wvwDNAv8FJNh7B\nBuXvGL4KO92bPMWER5HawTRWX+9u92VIJB4kkqNIjzJYW7J077Xn2GAcMlbL65Ia6W7mF4iD\nt/YvT1HbkLUA08ITHDDts1gL8tUlo0hXMexqY0ZMIrGLWtv0dS1gNaBiA4nEg0RyFInNwyuf\nJNlmAAAcVElEQVR5pkQaRepml2Iz3c0vkDPsS0XvglLfyq8/LiEUDAYF8UtldEGBjT2wuX5G\nkRrAOIbiEpNIYqE01K7COmIn3BZEIvEgkZxE6sJ+0HJtCN8o0nsA0/vY6dYx3r70d3Xpx6AC\npKG5GjCO0FVigxcSn4B+PQvZhJHsIpFRJHap6aieJNjc3Oz8zAWzSDtwn3Xv2UyLry3yWEIi\n8SCRnERizYA+E2evXi0vSidfwtsA43g3EWLrk6b9TRrlwtjAdZmWYgXAXDzryjROrg1OVMYj\njCKJi5VpeTIfAEx1bgzNIrGrVdp8KQHbpzHOl3S1XSKR7CGRnES6bmo9lmvVsnMSwHr8fJaO\nZ0uc+lxhPB87pywsAUhRx+IeYjvz4f/f3rmHZ1HdeTyu7bru7rM+rdXt9rZ1b+3a3Xbb7e7T\n7dZ2u+t27UlCQgiEiAYIckkh3C9iowUDhqJSQwzIRasCJVJQAyEmEAyKoFAwQAIi9wjmQhJy\n8SXwZuaPPb+5z3s5M/O+E/MSvp8/yJkzZ37vMO/5vHPOmTMzsjJRwbwi+4I+tG0T6Q1muYJ6\ncYjjtWDCLlJfLmNjdXdoFHCJcwQNiCQCIjmJRCNbNXpelTH8LRUylqXM5N7MzKcgRIC2n6Yd\nhcsT1T6SMikhX61oPVO03lAjFypDuwVqJ29PZiv11ibSFd4rS9MmJLXRzUsno3+ujl0keRff\naoYyDVGqHsq7amejbBYORBIBkZxEupLG2BT1oErb6T6LkUq6xvArSDNQo9+P1EePY8za3NDZ\nc3oTzclRBs0kGsF4oLyp96MdD9JYhlKS7kRMXfVhoPtwEQ3vqVMsbCKp44RFhzoCZ8robsHl\nLo5FiEjSHL7diJW76rY8SrHKXETQgEgiIJLjqB1dLJq6r/Vqc81MxmgofNuVa/Kl4Yw9qjXo\nTqUw9nj0xl2d5f5a7qR6o0TnFEternqQgousBbW7Bu0iKQ1FgzluanaISHJHriVCiesJQhBJ\nzOAUqXHu3LnmOaJo7lzBfTuOIl16wKx4hWeUP9sk/nOeod+noXRoauWo7MsyAxTpx6+r0PRB\n39XgGsO5TH1UOkQkudq8b73YVcUOFUnuMD45q8KDRxBJyOAUyQsdlZWV+n8qyNNGra2trFS7\nIB9N1ire/ZWS9EtFpE5e0Lzo08uX3pGj0/HK3DH8rJU8oeQDS27dkyRY5oK9ltp8fhX1fJLz\n13foOW08tu1pJZ0bp9GF3bHFLvpHkZEaSvKyhuQUbHWYbRsCRBIBkVzQt3ftgkkzf/0mVaSu\nNdOnLj3uuEkYwZa28G8m0NQVdk4ItrY7XRySOpq91WlfgEgiIBJwCUQSAZGASyCSiBtGpEAU\nvHS3B/YDBvpjIZKIG0akMSwyvj0ivN8/YKA/FiKJgEjXzQcM9MdCJBE3jEggXiCSCIgEXAKR\nREAk4BKIJAIiAZcEWiBSdCAScAlEEgGRgEsgkgiIBFwCkURAJOASiCQCIgGXQCQREAm4BMPf\nIiAScAlEEgGRgEsgkgiIBFwCkURAJOASiCQCIgGXQCQREAm4BCKJgEjAJRBJBEQCLoFIIiAS\ncAlEEgGRvEBPzh/ofRgwIJIIiOSF61KklZl5ITl9B4ryMoZNKj3jKQ5EEgGRvHA9ihTMCnmI\nvnxqov6ooRXXIm8TEYgkAiJ54XoUaV/o2yh2DzEf2lXo/A5aA4gkAiJ54dKBAwcGeh88cjkn\nRKRjaYwlP/5mQ+0zydykze4jQSQREGlQI9VNZ3aRro1hbOg+JVmfztgw93JAJBEQafBy+LXi\nPKUBZxVpGzPfeLuV0XvZ3QKRRECkwYvxokCLSBI/Ic3QH7B/NZOxYtfhIJIIiBTO1fe2llWf\nVtNX3i0v230qapf87Jubttd1RFtr0ntu35by/Y1BW2aw7o2yiv322ikd3/HKll3nwt4l8cnb\nZVV6+tJbr/9+z6lgaJEwIon0IV/cbSxVlJZucN55DYgkYrCK9BTvUWu7Ko3klWeblr+Sp89G\n2+gE1Tlpk1oBZ1zkHYoXM5R0/jG1wDv6qJ1SUm6YpaxNXeFQBdpL1DBsVJlZ/QPPq5+TXmQ+\n7753wyi14LhK3d2jjI2X5bfvV/4QdbPVImO3On1zOyuJ6TaR1vHd/ST6JiIgkojBKlKt2fw/\nR9WuSMufzFhO1FcHkR7BhfrveNbHPdP1dLL6sku7SDuNceTsj0T70mC+P5nN7dUyz+YYeen6\nKeLiOLPgLK0aKiK9SQNsikh9JWaRia6qxlKbSPP5L4SbrSIAkUQMVpE6edV7Tk1uV84FkpEd\nvVfA9RhdzNhDy1bOpW1mz2HswaLVBVSLf6GcSmwiHUlm6YUvLJ9GRWcK3uvVMozOWlMWzlfU\nKVUzG7Mpc1Zhfjr9VcfUW5TXp49f8JhScLJabUmkU8rLzkkkaSmlhs5cOIH+5gj91bCLlGP+\npngFIokYrCLJM3jlV1NK3WNqndvLU3ujbsP1SGap5STFu0rdZWvJnwZqGyrTaawiZT3ACql3\nJL0zlAlfR/Q0X/0kfevSW9yaFKVH1cd3jy2jmnO5mM4t9JFSAelznCelA/TWo1XK1lykMXks\n5cn3PqCK8AbPH1FLzb6GXJ6c4+K9fDaRAnyjlXLfnsXj0odPWnHMeWsLEEnEoBWJdwaY+s3w\nSpnGWKWSfk7YRThB7uxQ06sp/YKapnOa0vyyisTYL7UOz2s8/XbUoFImb9BpPZ5yXrKWElVK\nhVZ5SmuF7ud/p2o71zqaO/0xpY7SJ408pWb38FijW7U0nTXfcj4QNpEa+TZlF2bqjcNFgm/n\n6pUQukZ0hWYJ6e7wVDzQ0u6p/JVWb8XbWryVb+/xVLyzxdvR6brsqfgn/OhE/dXsV5GO69Ws\nhXdhFjO2VMmdwtjD0bchPWZpu1tPTSftJ7ibqh8l7CJptVvu4emNUYN2MHP+AN8X9golpjI2\nXP997+FdrbX87wIu+Tl9q/d4yd9SQhGpTst9naff1Ytc4NvNFBwBDZtIJ3mAl0eZ3azxrdH3\nuyWUEWE54NMk6thxv4rUl8XYs5SoYaxwmzbE0JUsnBRDerympdt5ukRfwdt2yjCxTaTHje1y\ntNURodbUi1paCgaDdDjoxLDWKPEIYwW8JcQbk08YeVKeNrpAIs3Tc6dZ0srpdUiv7IRNJPp5\n4J+TvbrmSMUy6vvNjjqK3hv6eufOEZ3R3vwcka4OT8X5b66n8oFWb8XbWryVb+/2VJyfkTyV\n93h0evjRGZgzkryEsQn0dzljr9LAHTWU9vG/56JvcsLy40/1Xx8zp9ZhuEimkeNFIpFmQ+vs\nWdSyazSWmo4dO6XW8VqzyEt8kZrRJNLvtbwrKYytN4vs4auORv9cDZtIB5Xz0GK1zVtPgxpV\n0bYLA30kEYO2jyTv4LWkjf+dyNgJ6X51Xgzv+IwW9M9PWDwjkfboKyKK9I6xnVik31LdfbSq\nzZL1LGMpoacCmrtzwr77R2RVJF3DBp4+aBa5wBd3Rf9cDZtI7zNL61WJPd7FeIUKRBIxeEVq\nV6tZJ2PDgnIRY0/zvHxLcy0CpMd5LR2wqhJRpJPGdmKRrk5T+yO5T1XrB4P32XJCi/3OGB1R\nqNM+nyr7aS1vLwtju+C/o2ITiXqOKeYF6UK+eMExggZEEjF4RSJrnlEqX4Hyc89rU3eyeJKm\nN5HOG9uJRZKvlaVr1T71UfXkMj/CMMEaxtItZ4eLWrPrqNYqJarCRXK+DcIm0hm+ieV+2Wq+\nuNMxggZEEjGIRXqRnwSU1txGWT7Pa0yz/C7vnYsqQz+JxL+UHY/rLpVSk+7hCPMLeAsw2dLc\no/EIGoknkfSLVNTcy77fhkeR2pjtgmyDpQPmCEQSMYhFUqvgNKWrIY2in17+oz9ftEW/icTp\nPbzh4RT9HLKIsQdCC2xitsu6B5h69rSKRGMlbiYz2LCJRBe1VprrmpiLPdeBSCIGsUhBXmmq\nAyksjYaIeSfpN3T1RvgL3p8iEZ3rUnn7LaAMNiSbj0sIBgIBSd6pjS5o0NgDTfCzinTcMvDg\nGptI8nSlpatDHbEKt4EgkohBLBJ16Z86xNgcSlcwNq4nWTj43U8ifXL5snkMytShuWrbCN0W\n3nkKKq3P18zNinkm/QJYRaJLTeVmkcCFCxecn7lgF4m3dB80e2Ll9mFAMRBJxGAW6Q3Gcl7S\nroZSl6OSVynhYG+/iMTPPkOM7+QUU+Zb0MD1UqPEfOWKl5TF2FRj9wIjtPEIq0jyLG1anspy\nYy6uCLtIdLXKmGwo8fNTmvMlXWOXIFJ0BrNINCFnLGPK1GrqJI0VD373k0hl1vZYrbYwm7EU\nfSzuDD/PrJCViQrmFdkX9KFtm0hvMMsV1ItDGFsm/P8o2EXqy+XHQXeHRgGXOEfQgEgiBrNI\nsvLEguQeJb1EGTMTP6GgX0SikbFp2lG4PFHtIymTEvLVitYzResNNXKhMtT7nuSdvBWardRb\nm0hX+M9BmjbZro1uXjKvZUXFLpK8i281o5lSUvVQfnSi3uUYBkQSMahFWkvuTFXTyl1JwsHv\nfhKpjx7HmLW5obPn9Caak6MMmkl0++AD5U29H+14kKcKlZLr6UrTqg8D3YeLaGfVeRU2kZS5\nrKzoUEfgTBndLbjcxUEIEUmaw7cbsXJX3ZZHKVaZiwgaEEnEoBaJBqX02/s+ovQj4vL9M2pX\nl8osTFFvlOicYsnLVQ9ScJG14Mvq1naRlIaiwRw3NTtEJLkj1xKhxPUEIYgkZlCLdC3D+GGX\nJbr9dIu4fD8Nf+/LMmtukX78ugpNH/RpeME1hnOZ+qh0iEhytXnferGrih0qktxhfHJWhQeP\nIJKQQS2SXLVu3Tp9B2p42uFYdlRWVur/vyBPGxW4trJS6Y008cywkrL8lrY6atxX5o5J4f2R\nCSUfWHLrniTBMhfstdTm86uo55Ocv954MlEb/yTbrYidG6fRhd2xxS76R5GRGkrysobkFGzt\n8bQZRBIxuEVKIIItbeHfTKCpK+ycEGxtd7o4JHU0e6vTvgCRREAk4BKIJAIiAZdAJBE3okjR\nbuf10vP+tKIm0MdCJBE3okhjWGQEj9QasKgJ9LEQSQREgkgugUgibkSRQExAJBEQCbgEIomA\nSMAlgRaIFB2IBFwCkURAJOASiCQCIgGXQCQREAm4BCKJgEjAJRBJBEQCLoFIIiAScAlEEgGR\ngEt6210/uUvhqrf7BqV2b1VL9uhdV7u38t3XnMtYuNLuTbxeb++W72sXHE2IBIAPQCQAfAAi\nAeADEAkAH4BIAPgARALAByASAD4AkQDwAYgEgA9AJAB8ACIB4AMQCQAfgEgA+ABEAsAHIBIA\nPgCRAPABiASAD0AkAHwAIgHgAxAJAB+ASAD4AEQCwAcgEgA+AJESl+OlkzKHTSw5GnuBuML3\nvrM0b2Rq9tTVp/onvsaezMzn+iV834GivIxhk0rPxBLdMfyV7YWj04eOWVSlPWgQIiUqXU/q\nr6ItivwYRscC8YU/ONp4F+5iwRNGY46vcSmLseX9Ef7URL3ECm+PmXQVvnakXmDUPiUDIiUo\nPfnmS52nRHrCp2OB+MJvtb5VeqzHRxO73zvpERaLSM7hdw8xSxT2+R3+d9aj8xrlQKQEpZB+\n7MreP7TxfqoJsRSIK/wfknn+hFePnKhSfpvzvf6ou927LSwmkRzDH0tjLPnxNxtqn6H/yGaf\nw++jvZ7x+sFD5TN5IrlOhkiJymH+BU3uoFR7Hk8e8V4grvAS/SSvDCrpo9TGe93n3dc4PSQm\nkRzDXxvD2FC1zVWfztgwb68ecDw6D9F5SFKSr9PPjASREpU5jA1pVJPnUhmb471AXOGP8eqx\nUNIW6vmP+mgptEhc8TV6J7GYRHIMv41HrdbS1Ejd52v4D3jEJfpCEV84B5ESlEvW6lXMF0I7\nKY4F4gv/LM8zR7uW8aXTvsbXWMlYXqZ3kRzDS/yENENX/yr/hGJfw5Om7+kL1MzbDpESlHL+\n7TToC/V8YavXAvGFL2As1zwHVfMSb/kaX+UAY2lnYhi1cwz/Ic/bbSxVlJZu8DX8ep73sb5w\nkS9sgEgJymLerjeGmoIZjC3yWiC+8LzJtcBcoqbMNl/jK3TwrvyrcgwiOYZfx1iqt3cfeQpP\nptXpC3XquB1ESkh4RX7YXJrHW0BeC8QXnreGSs2ld3ld2e9rfEJawNh8KRaRHMPP5y07jzG9\nhD/Cj8dafeF5tQsGkRKRIO/iLjMXn+JNoD5vBeILL3d3d1teDbiG15WL7sO73Dve0xjRKscg\nknP4HMaKvMX0FF6awliKdko6nMJY9lWIlJg08Zq72lxcxRdbvBWIL7ydDt66meBl1M5V/PPp\nasfLu0iO4QM0di/37Vk8Ln34pBXHvEV3s/eNIxlLLqm/fLm+hC5T1coQKTE5o3ZgdTaoI6xe\nCsQX3kbvLL6+xn10d/Gv5TP2NCW8i+QYvpHnlF2Yqc88WORthpObvT+XxwxSKygHIiUidBmn\n3FykKyEfeCsQX3grzbP56ulB99Hdxeddi7HK5BvvIjmGP8lzXh5lVvXxrb6GJy4O14Nnn1Qy\nIFIicoh/QW+ai7XWUSJ3BeILb9K3nWrMeG+zYl3Er0tmyfVKyrtIjuFpyJp3dLJX1xypWEZt\nr9lefgfcHJ3qTFPTRcplJoiUiBzSGt4aNXzxoLcC8YU3qPsFVZW8Jvex3cXvymHsJTUZm0jC\n8AeVCr5Ytb+efxSr8jO8Ov6dumpfU9O+57iww+k+E4iUiDTYL9zYrhC6KxBfeI32xVQhk9de\ndR/aXXzpCcamaWcJ7yI5hn+fdnuWPjxylE6pHsZKnI/OQX6Wm6BN/Dg9gbH7myBSYnKaf3kb\nzUXq757xViC+8CpvZ1GFnO7BULfxdzA2VJvMFoNIjuGP85yUs8YizeW+4GN4eSY/Hxl3O55M\nYexJiJSY0AjsGnNxNV9s8lYgvvBE11LSKHe3t9mq7uLncnmOafDexuP8j4ffAcfwNO5muYhK\nM5x2+hieTFthLpZyrVohUkLSZ78muIx/VUFvBeILz7nEmywsfbP3m0vdxB/DwpjqY/g2Zrsg\nS2213/sYfhezTT3czRf3QqTEZCJjBeZSAWMTvRaIL7zcM54Guzw0iDzFj08kx/BSJl2QNaBT\njJdZq07hN/J4jebieb64BSIlJryXn2W0qaSsiJNWxQXiCy8v4bVjmaeLR17ixymS4+5Pt6lA\n00orfAy/icezPBCGrlqVQaTEpMLawz3HlDtevBWILzx115fG0DuKYe9imLTqGJ73ax409748\nwgB2POFpQLzGXNzB6CZCiJSQNPMv53l94QW+EHpt3rFAfOGfYGxkDI8OimXvYhDJMXy90m3R\nkPj5Ka1Xdo9T+CZ7F0y9RRYiJSbzGBveoSYv86o2X01+XFdXd05UwKfwvRmMPduvu28Sy+O4\nnML35TI2VnenilluDPcl/BQe8YBe+ABTLlNBpMSE2vULlD7KtYXMeP7Gi9TiEhXwKfwpnihY\nbePj6MFi2X2TWERyDE8DazOaKSVVD2Us+WzkODGG38tT2eoZT9qTrTb0IFJiItGkgjl1gZ4/\nzLQ0JCxVJXIBn8LvCR8M8PY4V8fdN4lFJOejM4enR6zcVbflUaaMBfgaXqKhGParzYf2b/ol\npR7DU4QSl08mmZV4sv40KWtNjFjAp/Cb4xXJefcNYnrSqmP4jlzLvpd4HTZxCt/7mPXQzKUS\nEClR6S7Uv6gi41mftpoYqYBP4UvjFsl593Vie2SxY/gOo0BWhffhR6fwfRuH6QUyXlYagRAp\nYZGOPDVx2LCJy+qj1QPHAvGFj5N+ju98dBpK8rKG5BRsjeFXxkX4ztd/NSY9PeexV7U7TCAS\nAD4AkQDwAYgEgA9AJAB8ACIB4AMQCQAfgEgA+ABEAsAHIBIAPgCRAPABiASAD0AkAHwAIgHg\nAxAJAB+ASAD4AEQCwAcgEgA+AJEA8AGIBIAPQCQAfAAiAeADEAkAH4BIAPgARAJx0bFjXfHC\nX6/efLKfHo93vQCRQOwcn/qtm5I0bk/d4uHt5/fxLQr6bb8GAIgEYuXw/ybZ+eKqPrfbQiQA\nFIJP/HFSGD9w+/4UiAQAcSVFc+eWu/87e+hP7tCWvlLvbnOIBADn2v+p4qRt7FKWpROPfUnJ\nuNPdm9AhEgCcfMWa7+6xZHXPU/J+6upd6AerqqpO9tO+DQgQCcTADsWZ7JBhuk2fody1A7JH\nAw1EAt7p+w4ZMy5sjG45ZX/D9dDdYAIiAe+8QMJ8L/yykfQftGL7AOzRgAORgGcCX6HRukgv\nw6wgkfI/9R1KACAS8Ew56fJQpDVXb7/55pu/bc35sDjj7i9+9tY7v/vA8+3iqIGX7rv7tlu+\n9LNft/q3p58eEAl4ZjyJdMRNySMZxhSipKRbH/nEWJFpDn8X8OR6Wd5yu1Hu6euwlwWRgFck\natn90E3JjbfaJz78qFNfEyqStMBabvL1NwMWIgGvHKK6/oiLguWaF5/98p9qqVx9VahIz/F/\nbvrOz39wm1puU//seT8CkYBXVlBVr3Qu1/tlKvjN1R/3yVJ7+c8VQ85p60JEmnNL0s0zL/IF\nafNXqdi3+mnX+w+IBLzyGFV1FyMCW6ncj41BcqXx9qy2ECLSTUm36Ga23EnlTvm6x58CEAl4\nZSKv6H/kYjxgCQ0ctBiL0j/w5dnaQohISUnPGeXWuzzhJRYQCXgljVf0220599jHFJaouWN5\n8l5LqdGWQfNQkb5vDi90JqnDeNcXEAl45Ye8ot9ly4ks0roFCxbstJSaJhBpraXc1yESuBEg\nbe4IywkXKZQ0gUjNlnLfg0jgRiAtrI/kSqStn40u0tesBSESuCEYR7JcsuY0nzdgkUTqen/D\nvH9THIsi0g+spSESuCGYS0bURlmZbBepb++inB/9lXmyiiKSdUwCIoEbg5fJiIVRVv67VaTW\nKXck2Yki0n3WEBAJ3BA0khH/E2XlVy0ibfycIdAt/5xbORkiAWDhb8mM5oirTloGG167WXHo\n9vsWvnqCHuQwHSIBYOGh6G275aZI3cr56J6dxqVWiASAlYNkyJd6I6wJ3m2K9Awlp1lWQiQA\nbPyUHJkXYcU6y3UkeoLkP16zrMyHSABY2Ua63LQrLP/sFywifTsp5H70n0EkAKxIQ8iXv9wf\nkt32PevMBhq/e9iytv3zEAkAG63KNdY/22S7JfyDv7dNEfouTzFzbfBnuI4EQAjVykNVk/5r\nr6FS+zx6QMM3hxki5dIg+UF99eUMZYMHtUWIBACx9U/Us8/fTP5d7fGjO1ekKo9l+JfmjYZI\nlZTx15WKaYE1X1OL36U9SAgiAaBQe1tSGPe0yx8ZIkk/VvK+lTl9/L1k2Z00/J30d5OX0UqI\nBIBK48gQjW4rpXsr7jJmNlz8unXtv55rVp9xdw+tg0gA6Oy+9zOmKJ8ff1HJHGXOtWtOMdbe\n8ew19YosRAIgjLbnR/znXbf+xTd+krtNf1rQla4uc87DH6Z9/wuf+dw3R6ivI7v6m3/68zvv\nKaLk0ZqamtNqmdM8WWcNup9nNH0KO+8rEAkAH4BIAPgARALAByASAD4AkQDwAYgEgA9AJAB8\nACIB4AMQCQAfgEgA+ABEAsAHIBIAPgCRAPABiASAD0AkAHwAIgHgAxAJAB+ASAD4AEQCwAcg\nEgA+AJEA8AGIBIAPQCQAfAAiAeAD/w+sbfzYoVUw7AAAAABJRU5ErkJggg==",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "importance_matrix <- xgb.importance(colnames(X_train),model=gbdt_medium)\n",
    "\n",
    "df <- as_data_frame(importance_matrix)\n",
    "df.tf <- subset(df, Feature %in% unique(motif.class$class))\n",
    "df.notf <- subset(df, !(Feature %in% unique(motif.class$class)))\n",
    "tfclass.row <- c(\"TF_class\", unname(as.list(colSums(df.tf[!(colnames(df.tf) %in% c(\"Feature\"))]))) )\n",
    "names(tfclass.row) <- colnames(df)\n",
    "df.sum <- rbind(df.notf,tfclass.row)\n",
    "\n",
    "ggplot(data=df.sum, aes(x=reorder(Feature, Gain), y=Gain)) +\n",
    "    geom_bar(stat=\"identity\") +\n",
    "    coord_flip() +\n",
    "    theme_minimal(base_size = 30) +\n",
    "    labs(x = \"Feature\", y=\"Gain\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've checked out the features, we'll get back to the model. We're using 2 Rory functions that put together some stuff:\n",
    "\n",
    "* `make.pred.df.from.model`\n",
    "    * Takes in a model and runs `predict()` on it with the test data\n",
    "    * Makes a data frame where it tacks on the actual response test data\n",
    "    * Adds a column denoting the name of the model and returns it\n",
    "* `make.stats.df.from.preds`\n",
    "    * Takes in the data frame made by above function\n",
    "    * Uses the `roc()` and `coords` functions in succession to generate ROC numbers (sensitivity, specificity, etc) for a range of points from 0 to 1\n",
    "        * The range of points corresponds to the threshold for a prediction being classified as \"TRUE\"\n",
    "    * Generates Matthews correlation coefficients, also for a range of thresholds from 0 to 1\n",
    "        * Recall that these take into account TP/TN/FP/FN stuff\n",
    "    * Adds the MCC to the ROC curve numbers, adds the name of the model, and returns the whole thing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the model if necessary\n",
    "gbdt_medium <- xgb.load(\"saved_models/xgboost_TF_site_predict_joined.ALL.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "medium_pred_df <- make.pred.df.from.model(gbdt_medium, X_test, y_test)\n",
    "colnames(medium_pred_df)[1] <- \"ChIPseq.bound\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "medium_stat_df <- make.stats.df.from.preds(medium_pred_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just to clarify, here's the two data frames:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>ChIPseq.bound</th><th scope=col>Prediction</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>1        </td><td>0.4527620</td></tr>\n",
       "\t<tr><td>1        </td><td>0.4527620</td></tr>\n",
       "\t<tr><td>1        </td><td>0.2888901</td></tr>\n",
       "\t<tr><td>1        </td><td>0.2597707</td></tr>\n",
       "\t<tr><td>1        </td><td>0.3381387</td></tr>\n",
       "\t<tr><td>1        </td><td>0.3533445</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ll}\n",
       " ChIPseq.bound & Prediction\\\\\n",
       "\\hline\n",
       "\t 1         & 0.4527620\\\\\n",
       "\t 1         & 0.4527620\\\\\n",
       "\t 1         & 0.2888901\\\\\n",
       "\t 1         & 0.2597707\\\\\n",
       "\t 1         & 0.3381387\\\\\n",
       "\t 1         & 0.3533445\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "ChIPseq.bound | Prediction | \n",
       "|---|---|---|---|---|---|\n",
       "| 1         | 0.4527620 | \n",
       "| 1         | 0.4527620 | \n",
       "| 1         | 0.2888901 | \n",
       "| 1         | 0.2597707 | \n",
       "| 1         | 0.3381387 | \n",
       "| 1         | 0.3533445 | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "  ChIPseq.bound Prediction\n",
       "1 1             0.4527620 \n",
       "2 1             0.4527620 \n",
       "3 1             0.2888901 \n",
       "4 1             0.2597707 \n",
       "5 1             0.3381387 \n",
       "6 1             0.3533445 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "head(medium_pred_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th></th><th scope=col>threshold</th><th scope=col>sensitivity</th><th scope=col>specificity</th><th scope=col>ppv</th><th scope=col>npv</th><th scope=col>accuracy</th><th scope=col>MattCC</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>0</th><td>0.00       </td><td>1          </td><td>0          </td><td>0.008425261</td><td>NaN        </td><td>0.008425261</td><td>0          </td></tr>\n",
       "\t<tr><th scope=row>0.01</th><td>0.01       </td><td>1          </td><td>0          </td><td>0.008425261</td><td>NaN        </td><td>0.008425261</td><td>0          </td></tr>\n",
       "\t<tr><th scope=row>0.02</th><td>0.02       </td><td>1          </td><td>0          </td><td>0.008425261</td><td>NaN        </td><td>0.008425261</td><td>0          </td></tr>\n",
       "\t<tr><th scope=row>0.03</th><td>0.03       </td><td>1          </td><td>0          </td><td>0.008425261</td><td>NaN        </td><td>0.008425261</td><td>0          </td></tr>\n",
       "\t<tr><th scope=row>0.04</th><td>0.04       </td><td>1          </td><td>0          </td><td>0.008425261</td><td>NaN        </td><td>0.008425261</td><td>0          </td></tr>\n",
       "\t<tr><th scope=row>0.05</th><td>0.05       </td><td>1          </td><td>0          </td><td>0.008425261</td><td>NaN        </td><td>0.008425261</td><td>0          </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|lllllll}\n",
       "  & threshold & sensitivity & specificity & ppv & npv & accuracy & MattCC\\\\\n",
       "\\hline\n",
       "\t0 & 0.00        & 1           & 0           & 0.008425261 & NaN         & 0.008425261 & 0          \\\\\n",
       "\t0.01 & 0.01        & 1           & 0           & 0.008425261 & NaN         & 0.008425261 & 0          \\\\\n",
       "\t0.02 & 0.02        & 1           & 0           & 0.008425261 & NaN         & 0.008425261 & 0          \\\\\n",
       "\t0.03 & 0.03        & 1           & 0           & 0.008425261 & NaN         & 0.008425261 & 0          \\\\\n",
       "\t0.04 & 0.04        & 1           & 0           & 0.008425261 & NaN         & 0.008425261 & 0          \\\\\n",
       "\t0.05 & 0.05        & 1           & 0           & 0.008425261 & NaN         & 0.008425261 & 0          \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| <!--/--> | threshold | sensitivity | specificity | ppv | npv | accuracy | MattCC | \n",
       "|---|---|---|---|---|---|\n",
       "| 0 | 0.00        | 1           | 0           | 0.008425261 | NaN         | 0.008425261 | 0           | \n",
       "| 0.01 | 0.01        | 1           | 0           | 0.008425261 | NaN         | 0.008425261 | 0           | \n",
       "| 0.02 | 0.02        | 1           | 0           | 0.008425261 | NaN         | 0.008425261 | 0           | \n",
       "| 0.03 | 0.03        | 1           | 0           | 0.008425261 | NaN         | 0.008425261 | 0           | \n",
       "| 0.04 | 0.04        | 1           | 0           | 0.008425261 | NaN         | 0.008425261 | 0           | \n",
       "| 0.05 | 0.05        | 1           | 0           | 0.008425261 | NaN         | 0.008425261 | 0           | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "     threshold sensitivity specificity ppv         npv accuracy    MattCC\n",
       "0    0.00      1           0           0.008425261 NaN 0.008425261 0     \n",
       "0.01 0.01      1           0           0.008425261 NaN 0.008425261 0     \n",
       "0.02 0.02      1           0           0.008425261 NaN 0.008425261 0     \n",
       "0.03 0.03      1           0           0.008425261 NaN 0.008425261 0     \n",
       "0.04 0.04      1           0           0.008425261 NaN 0.008425261 0     \n",
       "0.05 0.05      1           0           0.008425261 NaN 0.008425261 0     "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "head(medium_stat_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a linear model on all features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're going to do something a little different; instead of training a boosted tree model where we add lots of trees together, we're going to make some linear models. As such, we'll re-brand our data as the \"linear\" data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_lin <- X_train\n",
    "y_train_lin <- y_train\n",
    "X_test_lin  <- X_test\n",
    "y_test_lin  <- y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll treat the TF classes differently here; we have to transform them to factors for the linear model, but that's fine. We're going to make a model called \"glm small\" where we use ALL the features, with each TF class as a factor variable. \n",
    "\n",
    "Note that this is logistic regression; we're using `family = binomial`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ERROR",
     "evalue": "Error in is.data.frame(x): object 'X_train_lin' not found\n",
     "output_type": "error",
     "traceback": [
      "Error in is.data.frame(x): object 'X_train_lin' not found\nTraceback:\n",
      "1. colnames(X_train_lin)",
      "2. is.data.frame(x)"
     ]
    }
   ],
   "source": [
    "tf.regressors <- colnames(X_train_lin)[colnames(X_train_lin) %in% unique(motif.class$class)]\n",
    "non.tf.regressors <-  colnames(X_train_lin)[!colnames(X_train_lin) %in% unique(motif.class$class)]\n",
    "\n",
    "tf.regressors.formula <- paste(\"as.factor(\", paste(tf.regressors, collapse=\") + as.factor(\"), \")\")\n",
    "non.tf.regressors.formula <- paste(non.tf.regressors, collapse=\" + \")\n",
    "all.regressors.formula <- paste(non.tf.regressors.formula, tf.regressors.formula, sep=\" + \")\n",
    "\n",
    "glm.formula <- paste(\"ChIPseq.bound ~ \", all.regressors.formula, sep='')\n",
    "\n",
    "glm.df.train <- as.data.frame(cbind(y_train_lin, X_train_lin)) %>% rename(\"cs_hit\" = \"ChIPseq.bound\")\n",
    "glm.df.test <-  as.data.frame(cbind(y_test_lin, X_test_lin)) %>% rename(\"cs_hit\" = \"ChIPseq.bound\")\n",
    "\n",
    "glm.all <- glm(as.formula(glm.formula), data=glm.df.train, family=binomial)\n",
    "glm.all$Model.Name <- \"glm small\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, we'll predict on the test set and assemble prediction stats in a data frame; we won't show that, but it looks a lot like the one above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glm.pred.df <- make.pred.df.from.glm(glm.all, glm.df.test)\n",
    "glm.stat.df <- make.stats.df.from.preds(glm.pred.df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Model on all Features Independently"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we'll do it for each feature individually and make a model for each, naming them accordingly. We'll also grab the stats for each and bind them together. So what we have is a BIG data frame comprised of like 17 data frames stacked on top of each other. BUT, each model has its name, so we can find the data for each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.regressors.df <- data.frame()\n",
    "\n",
    "for (this.regressor in colnames(X_train_lin)) {\n",
    "    \n",
    "    if (this.regressor %in% unique(motif.class$class)) {\n",
    "        glm.formula <- paste(\"ChIPseq.bound ~ \", \"as.factor(\", this.regressor, \")\",sep='')\n",
    "    } else {\n",
    "        glm.formula <- paste(\"ChIPseq.bound ~ \", this.regressor,sep='')\n",
    "    }\n",
    "\n",
    "    glm.df.train <- as.data.frame(cbind(y_train_lin, X_train_lin)) %>% rename(\"cs_hit\" = \"ChIPseq.bound\")\n",
    "    glm.df.test <-  as.data.frame(cbind(y_test_lin, X_test_lin)) %>% rename(\"cs_hit\" = \"ChIPseq.bound\")\n",
    "\n",
    "    glm.single <- glm(as.formula(glm.formula), data=glm.df.train, family=binomial)\n",
    "    glm.single$Model.Name <- paste(\"glm \", this.regressor, sep='')\n",
    "    \n",
    "    glm.pred.single.df <- make.pred.df.from.glm(glm.single, glm.df.test)\n",
    "    glm.stat.single.df <- make.stats.df.from.preds(glm.pred.single.df)\n",
    "    \n",
    "    stats.regressors.df <- rbind(stats.regressors.df, glm.stat.single.df)\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have all of our data frames stacked, we're going to filter out the 7 that are NOT a TF_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stats.regressors.df %>% \n",
    "filter(Model.Name %in% c(\"glm motifscore\",\n",
    "                         \"glm h_frac_16\",\n",
    "                         \"glm h_max_score_16\",\n",
    "                         \"glm w_frac_16\",\n",
    "                         \"glm w_min_score_16\",\n",
    "                         \"glm h_frac_20\",\n",
    "                         \"glm h_max_score_20\",\n",
    "                         \"glm w_frac_20\",\n",
    "                         \"glm w_min_score_20\",\n",
    "                         \"glm gc_content\",\n",
    "                         \"glm asinh_tss_dist\")) ->\n",
    "stats.regressors.filtered.df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## compare performance\n",
    "\n",
    "We're going to take the 7 single-factor linear models (stats.regressors.filtered), the gradient-boosted model (medium_stat), and the linear model with ALL regressors (glm.stat) and put them together. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all.stats.df <- rbind(\n",
    "    medium_stat_df,\n",
    "    glm.stat.df,\n",
    "    stats.regressors.filtered.df\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save(all.stats.df, file = \"/ssd/mrichard/data/joinedAllStats.ALL.Rdata\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using another Rory function, we're going to plot the family of MCC curves. Note that the function:\n",
    "\n",
    "* Plots a line using `geom_line`, with X = sensitivity and Y = PPV (positive predictive value)\n",
    "* Separates the points by `Model.Name`\n",
    "* Makes the scales 0-1\n",
    "* Makes the plot square\n",
    "* Labels the axes\n",
    "\n",
    "Also note that the `theme_minimal` command controls the background (white) and whatnot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "load(\"/ssd/mrichard/data/joinedAllStats.ALL.Rdata\")\n",
    "\n",
    "# Make a copy\n",
    "all.stats.orig <- all.stats.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Screen out some models for visualization\n",
    "all.stats.orig %>% \n",
    "filter(Model.Name %in% c(#\"glm motifscore\",\n",
    "                         #\"glm h_frac_16\",\n",
    "                         \"glm h_max_score_16\",\n",
    "                         #\"glm w_frac_16\",\n",
    "                         \"glm w_min_score_16\",\n",
    "                         #\"glm h_frac_20\",\n",
    "                         \"glm h_max_score_20\",\n",
    "                         #\"glm w_frac_20\",\n",
    "                         \"glm w_min_score_20\",\n",
    "                         #\"glm gc_content\",\n",
    "                         #\"glm asinh_tss_dist\",\n",
    "                         \"trees with classes\",\n",
    "                         \"glm small\"\n",
    "                         )) -> all.stats.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change labels and pull MCC\n",
    "all.stats.df$Model.Name[all.stats.df$Model.Name== 'glm small'] <- 'linear model (all regressors)'\n",
    "all.stats.df$Model.Name[all.stats.df$Model.Name== 'trees with classes'] <- 'gradient boosted model'\n",
    "all.stats.df$Model.Name[all.stats.df$Model.Name== 'glm h_max_score_16'] <- 'HINT best score seed 16'\n",
    "all.stats.df$Model.Name[all.stats.df$Model.Name== 'glm h_max_score_20'] <- 'HINT best score seed 20'\n",
    "all.stats.df$Model.Name[all.stats.df$Model.Name== 'glm w_min_score_16'] <- 'Wellington best score seed 16'\n",
    "all.stats.df$Model.Name[all.stats.df$Model.Name== 'glm w_min_score_20'] <- 'Wellington best score seed 20'\n",
    "\n",
    "max(all.stats.df$MattCC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create color palette\n",
    "myPalette <- c(\"black\",\"red\",\"blue\",\"green2\",\"darkorange\",\"purple\",\"magenta\",\"brown\",\"cyan\")\n",
    "myColors <- myPalette[1:6]\n",
    "names(myColors) <- c('gradient boosted model',\n",
    "                     'linear model (all regressors)',\n",
    "                     'HINT best score seed 16',\n",
    "                     'HINT best score seed 20',\n",
    "                     'Wellington best score seed 16',\n",
    "                     'Wellington best score seed 20'\n",
    "                    )\n",
    "colScale <- scale_colour_manual(name = \"Model.Name\",values = myColors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Reorder model names\n",
    "all.stats.df$Model.Name <- factor(all.stats.df$Model.Name,\n",
    "                                  levels = c('gradient boosted model',\n",
    "                                             'linear model (all regressors)',\n",
    "                                             'HINT best score seed 16',\n",
    "                                             'HINT best score seed 20',\n",
    "                                             'Wellington best score seed 16',\n",
    "                                             'Wellington best score seed 20'\n",
    "                    )\n",
    "                                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.mattcc.curve(all.stats.df) + theme_minimal(base_size = 15) + colScale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll do the same, only this time we'll plot the ROC curves. Recall that this is Sensitivity v. 1-Specificity, so Rory has named the axes accordingly. The function he wrote also allows you to label the area under the curve, but we don't do it here because....frankly, it'd be a mess with so many curves. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.roc.curve(all.stats.df, TRUE) + theme_minimal(base_size = 15) + colScale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last plot is a bit trickier, we're plotting \"recall\" v. precision. What are these exactly?\n",
    "\n",
    "* Recall: sensitivity; how good you are at only picking out \n",
    "* Precision:  PPV; how well you can pick out only the true positives (P/TP)\n",
    "\n",
    "What we're actually plotting here is PPV (positive predictive value) versus sensitivity. So I think we're just using synonyms here. The rest of the function just scales the plot from 0-1.\n",
    "\n",
    "The idea is we should try to maximize both things, so we can see that the GBM model does the best, again\n",
    "\n",
    "** Is it possible that the axis labels are switched?? **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.precrecall.curve(all.stats.df) + theme_minimal(base_size = 15) + colScale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Recreate figures as pngs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "png(\"./figures/joinedMCC.ALL.png\") \n",
    "plot.mattcc.curve(all.stats.df) + theme_minimal(base_size = 15) + colScale\n",
    "dev.off()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "png(\"./figures/joinedROC.ALL.png\")\n",
    "plot.roc.curve(all.stats.df, TRUE) + theme_minimal(base_size = 15) + colScale\n",
    "dev.off()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "png(\"./figures/joinedPreRec.ALL.png\")\n",
    "plot.precrecall.curve(all.stats.df) + theme_minimal(base_size = 15) + colScale\n",
    "dev.off()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model and get the importance matrix, plus combine the motif class info\n",
    "gbdt_medium <- xgb.load(\"saved_models/xgboost_TF_site_predict_joined.ALL.model\")\n",
    "motif.class$class <- lapply(motif.class$class, make.names, unique=TRUE)\n",
    "class(gbdt_medium)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance_matrix <- xgb.importance(colnames(X_train),model=gbdt_medium)\n",
    "\n",
    "df <- as_data_frame(importance_matrix)\n",
    "df.tf <- subset(df, Feature %in% unique(motif.class$class))\n",
    "df.notf <- subset(df, !(Feature %in% unique(motif.class$class)))\n",
    "tfclass.row <- c(\"TF_class\", unname(as.list(colSums(df.tf[!(colnames(df.tf) %in% c(\"Feature\"))]))) )\n",
    "names(tfclass.row) <- colnames(df)\n",
    "df.sum <- rbind(df.notf,tfclass.row)\n",
    "\n",
    "# Save to a png\n",
    "png(\"./figures/joinedImpMatrix.png\")\n",
    "ggplot(data=df.sum, aes(x=reorder(Feature, Gain), y=Gain)) +\n",
    "    geom_bar(stat=\"identity\") +\n",
    "    coord_flip() +\n",
    "    theme_minimal(base_size = 30) +\n",
    "    labs(x = \"Feature\", y=\"Gain\")\n",
    "dev.off()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
