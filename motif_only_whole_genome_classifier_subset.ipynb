{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>'RUnit'</li>\n",
       "\t<li>'GenomicRanges'</li>\n",
       "\t<li>'GenomeInfoDb'</li>\n",
       "\t<li>'IRanges'</li>\n",
       "\t<li>'S4Vectors'</li>\n",
       "\t<li>'BiocGenerics'</li>\n",
       "\t<li>'parallel'</li>\n",
       "\t<li>'stats4'</li>\n",
       "\t<li>'RPostgreSQL'</li>\n",
       "\t<li>'DBI'</li>\n",
       "\t<li>'caTools'</li>\n",
       "\t<li>'caret'</li>\n",
       "\t<li>'lattice'</li>\n",
       "\t<li>'stringr'</li>\n",
       "\t<li>'ROCR'</li>\n",
       "\t<li>'gplots'</li>\n",
       "\t<li>'pROC'</li>\n",
       "\t<li>'tidyr'</li>\n",
       "\t<li>'dplyr'</li>\n",
       "\t<li>'glmnet'</li>\n",
       "\t<li>'foreach'</li>\n",
       "\t<li>'Matrix'</li>\n",
       "\t<li>'xgboost'</li>\n",
       "\t<li>'ggplot2'</li>\n",
       "\t<li>'RColorBrewer'</li>\n",
       "\t<li>'stats'</li>\n",
       "\t<li>'graphics'</li>\n",
       "\t<li>'grDevices'</li>\n",
       "\t<li>'utils'</li>\n",
       "\t<li>'datasets'</li>\n",
       "\t<li>'methods'</li>\n",
       "\t<li>'base'</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 'RUnit'\n",
       "\\item 'GenomicRanges'\n",
       "\\item 'GenomeInfoDb'\n",
       "\\item 'IRanges'\n",
       "\\item 'S4Vectors'\n",
       "\\item 'BiocGenerics'\n",
       "\\item 'parallel'\n",
       "\\item 'stats4'\n",
       "\\item 'RPostgreSQL'\n",
       "\\item 'DBI'\n",
       "\\item 'caTools'\n",
       "\\item 'caret'\n",
       "\\item 'lattice'\n",
       "\\item 'stringr'\n",
       "\\item 'ROCR'\n",
       "\\item 'gplots'\n",
       "\\item 'pROC'\n",
       "\\item 'tidyr'\n",
       "\\item 'dplyr'\n",
       "\\item 'glmnet'\n",
       "\\item 'foreach'\n",
       "\\item 'Matrix'\n",
       "\\item 'xgboost'\n",
       "\\item 'ggplot2'\n",
       "\\item 'RColorBrewer'\n",
       "\\item 'stats'\n",
       "\\item 'graphics'\n",
       "\\item 'grDevices'\n",
       "\\item 'utils'\n",
       "\\item 'datasets'\n",
       "\\item 'methods'\n",
       "\\item 'base'\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 'RUnit'\n",
       "2. 'GenomicRanges'\n",
       "3. 'GenomeInfoDb'\n",
       "4. 'IRanges'\n",
       "5. 'S4Vectors'\n",
       "6. 'BiocGenerics'\n",
       "7. 'parallel'\n",
       "8. 'stats4'\n",
       "9. 'RPostgreSQL'\n",
       "10. 'DBI'\n",
       "11. 'caTools'\n",
       "12. 'caret'\n",
       "13. 'lattice'\n",
       "14. 'stringr'\n",
       "15. 'ROCR'\n",
       "16. 'gplots'\n",
       "17. 'pROC'\n",
       "18. 'tidyr'\n",
       "19. 'dplyr'\n",
       "20. 'glmnet'\n",
       "21. 'foreach'\n",
       "22. 'Matrix'\n",
       "23. 'xgboost'\n",
       "24. 'ggplot2'\n",
       "25. 'RColorBrewer'\n",
       "26. 'stats'\n",
       "27. 'graphics'\n",
       "28. 'grDevices'\n",
       "29. 'utils'\n",
       "30. 'datasets'\n",
       "31. 'methods'\n",
       "32. 'base'\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       " [1] \"RUnit\"         \"GenomicRanges\" \"GenomeInfoDb\"  \"IRanges\"      \n",
       " [5] \"S4Vectors\"     \"BiocGenerics\"  \"parallel\"      \"stats4\"       \n",
       " [9] \"RPostgreSQL\"   \"DBI\"           \"caTools\"       \"caret\"        \n",
       "[13] \"lattice\"       \"stringr\"       \"ROCR\"          \"gplots\"       \n",
       "[17] \"pROC\"          \"tidyr\"         \"dplyr\"         \"glmnet\"       \n",
       "[21] \"foreach\"       \"Matrix\"        \"xgboost\"       \"ggplot2\"      \n",
       "[25] \"RColorBrewer\"  \"stats\"         \"graphics\"      \"grDevices\"    \n",
       "[29] \"utils\"         \"datasets\"      \"methods\"       \"base\"         "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "libs <- c(\n",
    "    'RColorBrewer',\n",
    "    'ggplot2',\n",
    "    'xgboost',\n",
    "    'glmnet',\n",
    "    'dplyr',\n",
    "    'tidyr',\n",
    "    'pROC',\n",
    "    'ROCR',\n",
    "    'stringr',\n",
    "    'caret',\n",
    "    'caTools'\n",
    ")\n",
    "\n",
    "for (lib in libs) {\n",
    "        if (!require(lib, character.only = TRUE, quietly = TRUE)) {\n",
    "            install.packages(lib, repos='http://cran.us.r-project.org')\n",
    "        }\n",
    "}\n",
    "\n",
    "(.packages())\n",
    "\n",
    "source(\"my_R_functions/utility_functions.R\")\n",
    "source(\"my_R_functions/stat_functions.R\")\n",
    "source(\"my_R_functions/plot_functions.R\")\n",
    "source(\"/ssd/mrichard/github/BDDS/trenadb/src/utils.R\")\n",
    "source(\"/ssd/mrichard/github/BDDS/footprints/testdb/src/dbFunctions.R\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>motif</th><th scope=col>TF</th><th scope=col>class</th><th scope=col>family</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>MA0001.1                             </td><td>AGL3                                 </td><td>Other Alpha-Helix                    </td><td>MADS                                 </td></tr>\n",
       "\t<tr><td>MA0002.1                             </td><td>RUNX1                                </td><td>Ig-fold                              </td><td>Runt                                 </td></tr>\n",
       "\t<tr><td>MA0003.1                             </td><td>TFAP2A                               </td><td>Zipper-Type                          </td><td>Helix-Loop-Helix                     </td></tr>\n",
       "\t<tr><td>MA0004.1                             </td><td>Arnt                                 </td><td>Basic helix-loop-helix factors (bHLH)</td><td>PAS domain factors                   </td></tr>\n",
       "\t<tr><td>MA0005.1                             </td><td>AG                                   </td><td>Other Alpha-Helix                    </td><td>MADS                                 </td></tr>\n",
       "\t<tr><td>MA0006.1                             </td><td>Ahr::Arnt                            </td><td>Basic helix-loop-helix factors (bHLH)</td><td>PAS domain factors                   </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|llll}\n",
       " motif & TF & class & family\\\\\n",
       "\\hline\n",
       "\t MA0001.1                              & AGL3                                  & Other Alpha-Helix                     & MADS                                 \\\\\n",
       "\t MA0002.1                              & RUNX1                                 & Ig-fold                               & Runt                                 \\\\\n",
       "\t MA0003.1                              & TFAP2A                                & Zipper-Type                           & Helix-Loop-Helix                     \\\\\n",
       "\t MA0004.1                              & Arnt                                  & Basic helix-loop-helix factors (bHLH) & PAS domain factors                   \\\\\n",
       "\t MA0005.1                              & AG                                    & Other Alpha-Helix                     & MADS                                 \\\\\n",
       "\t MA0006.1                              & Ahr::Arnt                             & Basic helix-loop-helix factors (bHLH) & PAS domain factors                   \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "motif | TF | class | family | \n",
       "|---|---|---|---|---|---|\n",
       "| MA0001.1                              | AGL3                                  | Other Alpha-Helix                     | MADS                                  | \n",
       "| MA0002.1                              | RUNX1                                 | Ig-fold                               | Runt                                  | \n",
       "| MA0003.1                              | TFAP2A                                | Zipper-Type                           | Helix-Loop-Helix                      | \n",
       "| MA0004.1                              | Arnt                                  | Basic helix-loop-helix factors (bHLH) | PAS domain factors                    | \n",
       "| MA0005.1                              | AG                                    | Other Alpha-Helix                     | MADS                                  | \n",
       "| MA0006.1                              | Ahr::Arnt                             | Basic helix-loop-helix factors (bHLH) | PAS domain factors                    | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "  motif    TF        class                                 family            \n",
       "1 MA0001.1 AGL3      Other Alpha-Helix                     MADS              \n",
       "2 MA0002.1 RUNX1     Ig-fold                               Runt              \n",
       "3 MA0003.1 TFAP2A    Zipper-Type                           Helix-Loop-Helix  \n",
       "4 MA0004.1 Arnt      Basic helix-loop-helix factors (bHLH) PAS domain factors\n",
       "5 MA0005.1 AG        Other Alpha-Helix                     MADS              \n",
       "6 MA0006.1 Ahr::Arnt Basic helix-loop-helix factors (bHLH) PAS domain factors"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "load(\"Rdata_files/motif_class_pairs.Rdata\")\n",
    "head(motif.class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "load(\"/ssd/mrichard/data/joined.motifs.only.9.Rdata\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've got our fully-annotated dataset all together. We also have an `Rdata` file with motif-class pairings that we'll use somewhere later. Now we want to actually build a model!\n",
    "\n",
    "Skip the part where we take out things without hits for ChipSeq, HINT, or Wellington; just change the names to ones that are okay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "colnames(motifs.only) <- make.names(colnames(motifs.only), unique=TRUE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our modeling, we also don't need any info on:\n",
    "\n",
    "* Motif location (start/end/strand/chromosome)\n",
    "* Motif name\n",
    "* Sequence\n",
    "* P-value of the FIMO hit\n",
    "\n",
    "So we'll get rid of these. We'll also divide into 3 datasets:\n",
    "\n",
    "1. Validation: Just chromosomes 2 and 4\n",
    "2. Testing: Just chromosomes 1, 3, and 5\n",
    "3. Training: All the other chromosomes\n",
    "\n",
    "Then we'll get rid of the original dataset and the intermediate dataset with only hits. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cols_to_drop <- c('motifname', 'chrom', 'strand', 'loc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "motifs.only %>%\n",
    "    filter(chrom %in% c(\"2\",\"4\")) %>%\n",
    "    select(-one_of(cols_to_drop)) ->\n",
    "    val_df\n",
    "\n",
    "motifs.only %>%\n",
    "    filter(chrom %in% c(\"1\",\"3\",\"5\")) %>%\n",
    "    select(-one_of(cols_to_drop)) ->\n",
    "    test_df\n",
    "\n",
    "motifs.only %>%\n",
    "    filter(!(chrom %in% c(\"1\",\"2\",\"3\",\"4\",\"5\"))) %>%\n",
    "    select(-one_of(cols_to_drop)) ->\n",
    "    train_df\n",
    "\n",
    "remove(motifs.only)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purposes of our model-building, we'll turn each dataset into a matrix, which is the input format we need. \n",
    "\n",
    "We'll also separate out the ChipSeq hit, which is our Y (response) and everything else, which serve as our predictors. This ends up with nearly 2 million points in the training set, and a bit over half a million points in the testing set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val_df %>% \n",
    "    select(-cs_hit) %>%\n",
    "    as.matrix ->\n",
    "    X_val\n",
    "\n",
    "val_df %>% \n",
    "    select(cs_hit) %>%\n",
    "    as.matrix ->\n",
    "    y_val\n",
    "\n",
    "test_df %>% \n",
    "    select(-cs_hit) %>%\n",
    "    as.matrix ->\n",
    "    X_test\n",
    "\n",
    "test_df %>% \n",
    "    select(cs_hit) %>%\n",
    "    as.matrix ->\n",
    "    y_test\n",
    "\n",
    "train_df %>% \n",
    "    select(-cs_hit) %>%\n",
    "    as.matrix ->\n",
    "    X_train\n",
    "\n",
    "train_df %>% \n",
    "    select(cs_hit) %>%\n",
    "    as.matrix ->\n",
    "    y_train\n",
    "\n",
    "remove(val_df, test_df, train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>19819409</li>\n",
       "\t<li>26</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 19819409\n",
       "\\item 26\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 19819409\n",
       "2. 26\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] 19819409       26"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>5892439</li>\n",
       "\t<li>26</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 5892439\n",
       "\\item 26\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 5892439\n",
       "2. 26\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] 5892439      26"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>19819409</li>\n",
       "\t<li>1</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 19819409\n",
       "\\item 1\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 19819409\n",
       "2. 1\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] 19819409        1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>5892439</li>\n",
       "\t<li>1</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 5892439\n",
       "\\item 1\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 5892439\n",
       "2. 1\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] 5892439       1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dim(X_train)\n",
    "dim(X_test)\n",
    "dim(y_train)\n",
    "dim(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## gbdt\n",
    "\n",
    "Here's where we actually do the machine learning! We're going to use the `xgboost` package, which is \"Extreme Gradient Boosting\". It's a technique where you're progressively adding things to your model to \"boost\" its performance, at least in my recollection. Seems like the way we're boosting it is by adding trees...I'll have to look more at this. It looks like you can also boost with linear models, but we're using the trees, which are the default. \n",
    "\n",
    "We specify a handful of parameters that do the following (in order):\n",
    "\n",
    "* Set the objective to \"binary:logistic\", telling it to do classification instead of regression or \"ranking\"\n",
    "* Set the maximum tree depth to 7 rather than the default, 6.\n",
    "* Set the contribution of each additional tree to 0.005; this is conservative, it's to prevent overfitting, so we have to add more things, but each particular things adds less to the overall model\n",
    "* Set the evaluation metric to Area Under the Curve; essentially, you do a ROC curve and try to maximize area underneath it. \n",
    "    * Note that you can set multiple metrics; things like error or cumulative gain\n",
    "    \n",
    "Other than the `param` list, we also specify:\n",
    "\n",
    "* The data are our training set\n",
    "* The real labels are our training response\n",
    "* We are permitting 200 rounds of iterations; I'm guessing this is a large number, based on our small eta number\n",
    "* Verbose is FALSE, so it won't print output; it could also be \"1\" or \"2\" for different output amounts\n",
    "* Missing is NA, so consider missing values to be NA; could be 0, but this is the default, as well as our value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note: This takes quite a while (several hours) with our enlarged dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14:24:59] Tree method is automatically selected to be 'approx' for faster speed. to use old behavior(exact greedy algorithm on single machine), set tree_method to 'exact'\n"
     ]
    }
   ],
   "source": [
    "param <- list(\"objective\" = \"binary:logistic\",\n",
    "          \"max.depth\" = 7,\n",
    "          \"eta\" = 0.005,\n",
    "          \"eval.metric\" = \"auc\"\n",
    "          )\n",
    "\n",
    "gbdt_medium <- xgboost(\n",
    "    params = param,\n",
    "    data = X_train,\n",
    "    label = y_train,\n",
    "    nround = 200,\n",
    "    verbose = FALSE,\n",
    "    missing = NA\n",
    ")\n",
    "\n",
    "gbdt_medium$Model.Name <- \"trees with classes\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "TRUE"
      ],
      "text/latex": [
       "TRUE"
      ],
      "text/markdown": [
       "TRUE"
      ],
      "text/plain": [
       "[1] TRUE"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "xgb.save(gbdt_medium, \"saved_models/xgboost_TF_site_predict_motif_only.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a gradient-boosted classifier called \"trees with classes\" that we've trained. We'll use the model in a moment, but first we're taking our motif class pairs and using `make.names` so that they match the ones we changed earlier...this will be important in a second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "motif.class$class <- lapply(motif.class$class, make.names, unique=TRUE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're pulling out the \"importance matrix\"; we basically give the names of the features in the first argument and the model in the second argument. That's pretty straightforward, and the matrix is only 18 x 4 (number of features is 18). \n",
    "\n",
    "Given it's pretty small, here it is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>Feature</th><th scope=col>Gain</th><th scope=col>Cover</th><th scope=col>Frequency</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>asinh_tss_dist                       </td><td>7.384477e-01                         </td><td>3.667133e-01                         </td><td>2.679921e-01                         </td></tr>\n",
       "\t<tr><td>gc_content                           </td><td>1.338732e-01                         </td><td>2.567867e-01                         </td><td>2.925591e-01                         </td></tr>\n",
       "\t<tr><td>motifscore                           </td><td>6.977949e-02                         </td><td>2.206148e-01                         </td><td>2.240551e-01                         </td></tr>\n",
       "\t<tr><td>Basic.helix.loop.helix.factors..bHLH.</td><td>3.545088e-02                         </td><td>3.247282e-02                         </td><td>4.814961e-02                         </td></tr>\n",
       "\t<tr><td>C2H2.zinc.finger.factors             </td><td>1.262985e-02                         </td><td>8.628426e-02                         </td><td>6.086614e-02                         </td></tr>\n",
       "\t<tr><td>Tryptophan.cluster.factors           </td><td>5.577724e-03                         </td><td>1.749579e-02                         </td><td>3.775591e-02                         </td></tr>\n",
       "\t<tr><td>Basic.leucine.zipper.factors..bZIP.  </td><td>1.784156e-03                         </td><td>3.158017e-03                         </td><td>1.909449e-02                         </td></tr>\n",
       "\t<tr><td>MADS.box.factors                     </td><td>1.243737e-03                         </td><td>1.256603e-02                         </td><td>1.996063e-02                         </td></tr>\n",
       "\t<tr><td>Zinc.coordinating                    </td><td>8.723601e-04                         </td><td>2.899022e-03                         </td><td>1.413386e-02                         </td></tr>\n",
       "\t<tr><td>Zipper.Type                          </td><td>1.039902e-04                         </td><td>2.029667e-04                         </td><td>9.055118e-04                         </td></tr>\n",
       "\t<tr><td>Paired.box.factors                   </td><td>1.034257e-04                         </td><td>4.546193e-04                         </td><td>6.535433e-03                         </td></tr>\n",
       "\t<tr><td>Fork.head...winged.helix.factors     </td><td>4.605171e-05                         </td><td>7.060429e-05                         </td><td>3.503937e-03                         </td></tr>\n",
       "\t<tr><td>STAT.domain.factors                  </td><td>3.205680e-05                         </td><td>9.738922e-05                         </td><td>1.929134e-03                         </td></tr>\n",
       "\t<tr><td>Homeo.domain.factors                 </td><td>2.985212e-05                         </td><td>7.824111e-05                         </td><td>1.062992e-03                         </td></tr>\n",
       "\t<tr><td>Ig.fold                              </td><td>2.478597e-05                         </td><td>1.050921e-04                         </td><td>3.543307e-04                         </td></tr>\n",
       "\t<tr><td>Winged.Helix.Turn.Helix              </td><td>6.877557e-07                         </td><td>3.114922e-07                         </td><td>1.102362e-03                         </td></tr>\n",
       "\t<tr><td>SMAD.NF.1.DNA.binding.domain.factors </td><td>1.821753e-08                         </td><td>2.681847e-08                         </td><td>3.937008e-05                         </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|llll}\n",
       " Feature & Gain & Cover & Frequency\\\\\n",
       "\\hline\n",
       "\t asinh\\_tss\\_dist                        & 7.384477e-01                              & 3.667133e-01                              & 2.679921e-01                             \\\\\n",
       "\t gc\\_content                            & 1.338732e-01                            & 2.567867e-01                            & 2.925591e-01                           \\\\\n",
       "\t motifscore                            & 6.977949e-02                          & 2.206148e-01                          & 2.240551e-01                         \\\\\n",
       "\t Basic.helix.loop.helix.factors..bHLH. & 3.545088e-02                          & 3.247282e-02                          & 4.814961e-02                         \\\\\n",
       "\t C2H2.zinc.finger.factors              & 1.262985e-02                          & 8.628426e-02                          & 6.086614e-02                         \\\\\n",
       "\t Tryptophan.cluster.factors            & 5.577724e-03                          & 1.749579e-02                          & 3.775591e-02                         \\\\\n",
       "\t Basic.leucine.zipper.factors..bZIP.   & 1.784156e-03                          & 3.158017e-03                          & 1.909449e-02                         \\\\\n",
       "\t MADS.box.factors                      & 1.243737e-03                          & 1.256603e-02                          & 1.996063e-02                         \\\\\n",
       "\t Zinc.coordinating                     & 8.723601e-04                          & 2.899022e-03                          & 1.413386e-02                         \\\\\n",
       "\t Zipper.Type                           & 1.039902e-04                          & 2.029667e-04                          & 9.055118e-04                         \\\\\n",
       "\t Paired.box.factors                    & 1.034257e-04                          & 4.546193e-04                          & 6.535433e-03                         \\\\\n",
       "\t Fork.head...winged.helix.factors      & 4.605171e-05                          & 7.060429e-05                          & 3.503937e-03                         \\\\\n",
       "\t STAT.domain.factors                   & 3.205680e-05                          & 9.738922e-05                          & 1.929134e-03                         \\\\\n",
       "\t Homeo.domain.factors                  & 2.985212e-05                          & 7.824111e-05                          & 1.062992e-03                         \\\\\n",
       "\t Ig.fold                               & 2.478597e-05                          & 1.050921e-04                          & 3.543307e-04                         \\\\\n",
       "\t Winged.Helix.Turn.Helix               & 6.877557e-07                          & 3.114922e-07                          & 1.102362e-03                         \\\\\n",
       "\t SMAD.NF.1.DNA.binding.domain.factors  & 1.821753e-08                          & 2.681847e-08                          & 3.937008e-05                         \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "Feature | Gain | Cover | Frequency | \n",
       "|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n",
       "| asinh_tss_dist                        | 7.384477e-01                          | 3.667133e-01                          | 2.679921e-01                          | \n",
       "| gc_content                            | 1.338732e-01                          | 2.567867e-01                          | 2.925591e-01                          | \n",
       "| motifscore                            | 6.977949e-02                          | 2.206148e-01                          | 2.240551e-01                          | \n",
       "| Basic.helix.loop.helix.factors..bHLH. | 3.545088e-02                          | 3.247282e-02                          | 4.814961e-02                          | \n",
       "| C2H2.zinc.finger.factors              | 1.262985e-02                          | 8.628426e-02                          | 6.086614e-02                          | \n",
       "| Tryptophan.cluster.factors            | 5.577724e-03                          | 1.749579e-02                          | 3.775591e-02                          | \n",
       "| Basic.leucine.zipper.factors..bZIP.   | 1.784156e-03                          | 3.158017e-03                          | 1.909449e-02                          | \n",
       "| MADS.box.factors                      | 1.243737e-03                          | 1.256603e-02                          | 1.996063e-02                          | \n",
       "| Zinc.coordinating                     | 8.723601e-04                          | 2.899022e-03                          | 1.413386e-02                          | \n",
       "| Zipper.Type                           | 1.039902e-04                          | 2.029667e-04                          | 9.055118e-04                          | \n",
       "| Paired.box.factors                    | 1.034257e-04                          | 4.546193e-04                          | 6.535433e-03                          | \n",
       "| Fork.head...winged.helix.factors      | 4.605171e-05                          | 7.060429e-05                          | 3.503937e-03                          | \n",
       "| STAT.domain.factors                   | 3.205680e-05                          | 9.738922e-05                          | 1.929134e-03                          | \n",
       "| Homeo.domain.factors                  | 2.985212e-05                          | 7.824111e-05                          | 1.062992e-03                          | \n",
       "| Ig.fold                               | 2.478597e-05                          | 1.050921e-04                          | 3.543307e-04                          | \n",
       "| Winged.Helix.Turn.Helix               | 6.877557e-07                          | 3.114922e-07                          | 1.102362e-03                          | \n",
       "| SMAD.NF.1.DNA.binding.domain.factors  | 1.821753e-08                          | 2.681847e-08                          | 3.937008e-05                          | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "   Feature                               Gain         Cover        Frequency   \n",
       "1  asinh_tss_dist                        7.384477e-01 3.667133e-01 2.679921e-01\n",
       "2  gc_content                            1.338732e-01 2.567867e-01 2.925591e-01\n",
       "3  motifscore                            6.977949e-02 2.206148e-01 2.240551e-01\n",
       "4  Basic.helix.loop.helix.factors..bHLH. 3.545088e-02 3.247282e-02 4.814961e-02\n",
       "5  C2H2.zinc.finger.factors              1.262985e-02 8.628426e-02 6.086614e-02\n",
       "6  Tryptophan.cluster.factors            5.577724e-03 1.749579e-02 3.775591e-02\n",
       "7  Basic.leucine.zipper.factors..bZIP.   1.784156e-03 3.158017e-03 1.909449e-02\n",
       "8  MADS.box.factors                      1.243737e-03 1.256603e-02 1.996063e-02\n",
       "9  Zinc.coordinating                     8.723601e-04 2.899022e-03 1.413386e-02\n",
       "10 Zipper.Type                           1.039902e-04 2.029667e-04 9.055118e-04\n",
       "11 Paired.box.factors                    1.034257e-04 4.546193e-04 6.535433e-03\n",
       "12 Fork.head...winged.helix.factors      4.605171e-05 7.060429e-05 3.503937e-03\n",
       "13 STAT.domain.factors                   3.205680e-05 9.738922e-05 1.929134e-03\n",
       "14 Homeo.domain.factors                  2.985212e-05 7.824111e-05 1.062992e-03\n",
       "15 Ig.fold                               2.478597e-05 1.050921e-04 3.543307e-04\n",
       "16 Winged.Helix.Turn.Helix               6.877557e-07 3.114922e-07 1.102362e-03\n",
       "17 SMAD.NF.1.DNA.binding.domain.factors  1.821753e-08 2.681847e-08 3.937008e-05"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "importance_matrix <- xgb.importance(colnames(X_train),model=gbdt_medium)\n",
    "importance_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we're seeing here is the following:\n",
    "\n",
    "* Gain: contribution of a feature to the model (higher is better, and it's sorted by this)\n",
    "* Cover: Number of the observation related to the feature\n",
    "    * I'm not exactly sure what this means, but I don't think we're using it much\n",
    "* Weight (Frequency?): percentage of time the feature pops up in trees\n",
    "\n",
    "So we're most interested in the Gain, which basically tells us how important each feature is. We're going to turn this matrix into a data frame, then we'll:\n",
    "\n",
    "1. Separate out features that are TF classes and those that are not into 2 separate data frames\n",
    "2. Take the TF class data frame and do the following to make it into 1 feature, \"TF_Class\"\n",
    "    1. Select only the non-Feature columns, so the numeric ones that can be added\n",
    "    2. Sum those columns, so the gains are additive\n",
    "    3. Make them into a list\n",
    "    4. Take away their names so you can add \"TF_Class\" to them via concatenation\n",
    "    5. Add the correct names to each entry\n",
    "    6. Add that list as a row to the data frame\n",
    "\n",
    "Now we have the same importance matrix, but all the TF-Class features are added into one row. ***It's interesting to me that you can just do that...something to ask Rory about perhaps***.\n",
    "\n",
    "Now that we have that matrix, we'll make a barplot of the Gain variable to show which features had most gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAC91BMVEUAAAABAQECAgIDAwME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\nAABJRU5ErkJggg==",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "importance_matrix <- xgb.importance(colnames(X_train),model=gbdt_medium)\n",
    "\n",
    "df <- as_data_frame(importance_matrix)\n",
    "df.tf <- subset(df, Feature %in% unique(motif.class$class))\n",
    "df.notf <- subset(df, !(Feature %in% unique(motif.class$class)))\n",
    "tfclass.row <- c(\"TF_class\", unname(as.list(colSums(df.tf[!(colnames(df.tf) %in% c(\"Feature\"))]))) )\n",
    "names(tfclass.row) <- colnames(df)\n",
    "df.sum <- rbind(df.notf,tfclass.row)\n",
    "\n",
    "ggplot(data=df.sum, aes(x=reorder(Feature, Gain), y=Gain)) +\n",
    "    geom_bar(stat=\"identity\") +\n",
    "    coord_flip() +\n",
    "    theme_minimal(base_size = 30) +\n",
    "    labs(x = \"Feature\", y=\"Gain\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've checked out the features, we'll get back to the model. We're using 2 Rory functions that put together some stuff:\n",
    "\n",
    "* `make.pred.df.from.model`\n",
    "    * Takes in a model and runs `predict()` on it with the test data\n",
    "    * Makes a data frame where it tacks on the actual response test data\n",
    "    * Adds a column denoting the name of the model and returns it\n",
    "* `make.stats.df.from.preds`\n",
    "    * Takes in the data frame made by above function\n",
    "    * Uses the `roc()` and `coords` functions in succession to generate ROC numbers (sensitivity, specificity, etc) for a range of points from 0 to 1\n",
    "        * The range of points corresponds to the threshold for a prediction being classified as \"TRUE\"\n",
    "    * Generates Matthews correlation coefficients, also for a range of thresholds from 0 to 1\n",
    "        * Recall that these take into account TP/TN/FP/FN stuff\n",
    "    * Adds the MCC to the ROC curve numbers, adds the name of the model, and returns the whole thing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "medium_pred_df <- make.pred.df.from.model(gbdt_medium, X_test, y_test)\n",
    "colnames(medium_pred_df)[1] <- \"ChIPseq.bound\"\n",
    "medium_stat_df <- make.stats.df.from.preds(medium_pred_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just to clarify, here's the two data frames:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>ChIPseq.bound</th><th scope=col>Prediction</th><th scope=col>Model.Name</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>1                 </td><td>0.3073523         </td><td>trees with classes</td></tr>\n",
       "\t<tr><td>1                 </td><td>0.2833366         </td><td>trees with classes</td></tr>\n",
       "\t<tr><td>1                 </td><td>0.2114453         </td><td>trees with classes</td></tr>\n",
       "\t<tr><td>1                 </td><td>0.2114453         </td><td>trees with classes</td></tr>\n",
       "\t<tr><td>1                 </td><td>0.2431628         </td><td>trees with classes</td></tr>\n",
       "\t<tr><td>1                 </td><td>0.2633145         </td><td>trees with classes</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|lll}\n",
       " ChIPseq.bound & Prediction & Model.Name\\\\\n",
       "\\hline\n",
       "\t 1                  & 0.3073523          & trees with classes\\\\\n",
       "\t 1                  & 0.2833366          & trees with classes\\\\\n",
       "\t 1                  & 0.2114453          & trees with classes\\\\\n",
       "\t 1                  & 0.2114453          & trees with classes\\\\\n",
       "\t 1                  & 0.2431628          & trees with classes\\\\\n",
       "\t 1                  & 0.2633145          & trees with classes\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "ChIPseq.bound | Prediction | Model.Name | \n",
       "|---|---|---|---|---|---|\n",
       "| 1                  | 0.3073523          | trees with classes | \n",
       "| 1                  | 0.2833366          | trees with classes | \n",
       "| 1                  | 0.2114453          | trees with classes | \n",
       "| 1                  | 0.2114453          | trees with classes | \n",
       "| 1                  | 0.2431628          | trees with classes | \n",
       "| 1                  | 0.2633145          | trees with classes | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "  ChIPseq.bound Prediction Model.Name        \n",
       "1 1             0.3073523  trees with classes\n",
       "2 1             0.2833366  trees with classes\n",
       "3 1             0.2114453  trees with classes\n",
       "4 1             0.2114453  trees with classes\n",
       "5 1             0.2431628  trees with classes\n",
       "6 1             0.2633145  trees with classes"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "head(medium_pred_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th></th><th scope=col>threshold</th><th scope=col>sensitivity</th><th scope=col>specificity</th><th scope=col>ppv</th><th scope=col>npv</th><th scope=col>accuracy</th><th scope=col>MattCC</th><th scope=col>Model.Name</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>0</th><td>0.00              </td><td>1                 </td><td>0                 </td><td>0.132892          </td><td>NaN               </td><td>0.132892          </td><td>0                 </td><td>trees with classes</td></tr>\n",
       "\t<tr><th scope=row>0.01</th><td>0.01              </td><td>1                 </td><td>0                 </td><td>0.132892          </td><td>NaN               </td><td>0.132892          </td><td>0                 </td><td>trees with classes</td></tr>\n",
       "\t<tr><th scope=row>0.02</th><td>0.02              </td><td>1                 </td><td>0                 </td><td>0.132892          </td><td>NaN               </td><td>0.132892          </td><td>0                 </td><td>trees with classes</td></tr>\n",
       "\t<tr><th scope=row>0.03</th><td>0.03              </td><td>1                 </td><td>0                 </td><td>0.132892          </td><td>NaN               </td><td>0.132892          </td><td>0                 </td><td>trees with classes</td></tr>\n",
       "\t<tr><th scope=row>0.04</th><td>0.04              </td><td>1                 </td><td>0                 </td><td>0.132892          </td><td>NaN               </td><td>0.132892          </td><td>0                 </td><td>trees with classes</td></tr>\n",
       "\t<tr><th scope=row>0.05</th><td>0.05              </td><td>1                 </td><td>0                 </td><td>0.132892          </td><td>NaN               </td><td>0.132892          </td><td>0                 </td><td>trees with classes</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|llllllll}\n",
       "  & threshold & sensitivity & specificity & ppv & npv & accuracy & MattCC & Model.Name\\\\\n",
       "\\hline\n",
       "\t0 & 0.00               & 1                  & 0                  & 0.132892           & NaN                & 0.132892           & 0                  & trees with classes\\\\\n",
       "\t0.01 & 0.01               & 1                  & 0                  & 0.132892           & NaN                & 0.132892           & 0                  & trees with classes\\\\\n",
       "\t0.02 & 0.02               & 1                  & 0                  & 0.132892           & NaN                & 0.132892           & 0                  & trees with classes\\\\\n",
       "\t0.03 & 0.03               & 1                  & 0                  & 0.132892           & NaN                & 0.132892           & 0                  & trees with classes\\\\\n",
       "\t0.04 & 0.04               & 1                  & 0                  & 0.132892           & NaN                & 0.132892           & 0                  & trees with classes\\\\\n",
       "\t0.05 & 0.05               & 1                  & 0                  & 0.132892           & NaN                & 0.132892           & 0                  & trees with classes\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| <!--/--> | threshold | sensitivity | specificity | ppv | npv | accuracy | MattCC | Model.Name | \n",
       "|---|---|---|---|---|---|\n",
       "| 0 | 0.00               | 1                  | 0                  | 0.132892           | NaN                | 0.132892           | 0                  | trees with classes | \n",
       "| 0.01 | 0.01               | 1                  | 0                  | 0.132892           | NaN                | 0.132892           | 0                  | trees with classes | \n",
       "| 0.02 | 0.02               | 1                  | 0                  | 0.132892           | NaN                | 0.132892           | 0                  | trees with classes | \n",
       "| 0.03 | 0.03               | 1                  | 0                  | 0.132892           | NaN                | 0.132892           | 0                  | trees with classes | \n",
       "| 0.04 | 0.04               | 1                  | 0                  | 0.132892           | NaN                | 0.132892           | 0                  | trees with classes | \n",
       "| 0.05 | 0.05               | 1                  | 0                  | 0.132892           | NaN                | 0.132892           | 0                  | trees with classes | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "     threshold sensitivity specificity ppv      npv accuracy MattCC\n",
       "0    0.00      1           0           0.132892 NaN 0.132892 0     \n",
       "0.01 0.01      1           0           0.132892 NaN 0.132892 0     \n",
       "0.02 0.02      1           0           0.132892 NaN 0.132892 0     \n",
       "0.03 0.03      1           0           0.132892 NaN 0.132892 0     \n",
       "0.04 0.04      1           0           0.132892 NaN 0.132892 0     \n",
       "0.05 0.05      1           0           0.132892 NaN 0.132892 0     \n",
       "     Model.Name        \n",
       "0    trees with classes\n",
       "0.01 trees with classes\n",
       "0.02 trees with classes\n",
       "0.03 trees with classes\n",
       "0.04 trees with classes\n",
       "0.05 trees with classes"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "head(medium_stat_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a linear model on all features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're going to do something a little different; instead of training a boosted tree model where we add lots of trees together, we're going to make some linear models. As such, we'll re-brand our data as the \"linear\" data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_lin <- X_train\n",
    "y_train_lin <- y_train\n",
    "X_test_lin  <- X_test\n",
    "y_test_lin  <- y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll treat the TF classes differently here; we have to transform them to factors for the linear model, but that's fine. We're going to make a model called \"glm small\" where we use ALL the features, with each TF class as a factor variable. \n",
    "\n",
    "Note that this is logistic regression; we're using `family = binomial`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.regressors <- colnames(X_train_lin)[colnames(X_train_lin) %in% unique(motif.class$class)]\n",
    "non.tf.regressors <-  colnames(X_train_lin)[!colnames(X_train_lin) %in% unique(motif.class$class)]\n",
    "\n",
    "tf.regressors.formula <- paste(\"as.factor(\", paste(tf.regressors, collapse=\") + as.factor(\"), \")\")\n",
    "non.tf.regressors.formula <- paste(non.tf.regressors, collapse=\" + \")\n",
    "all.regressors.formula <- paste(non.tf.regressors.formula, tf.regressors.formula, sep=\" + \")\n",
    "\n",
    "glm.formula <- paste(\"ChIPseq.bound ~ \", all.regressors.formula, sep='')\n",
    "\n",
    "glm.df.train <- as.data.frame(cbind(y_train_lin, X_train_lin)) %>% rename(\"cs_hit\" = \"ChIPseq.bound\")\n",
    "glm.df.test <-  as.data.frame(cbind(y_test_lin, X_test_lin)) %>% rename(\"cs_hit\" = \"ChIPseq.bound\")\n",
    "\n",
    "glm.all <- glm(as.formula(glm.formula), data=glm.df.train, family=binomial)\n",
    "glm.all$Model.Name <- \"glm small\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, we'll predict on the test set and assemble prediction stats in a data frame; we won't show that, but it looks a lot like the one above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in predict.lm(object, newdata, se.fit, scale = 1, type = ifelse(type == :\n",
      "“prediction from a rank-deficient fit may be misleading”"
     ]
    }
   ],
   "source": [
    "glm.pred.df <- make.pred.df.from.glm(glm.all, glm.df.test)\n",
    "glm.stat.df <- make.stats.df.from.preds(glm.pred.df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Model on all Features Independently"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we'll do it for each feature individually and make a model for each, naming them accordingly. We'll also grab the stats for each and bind them together. So what we have is a BIG data frame comprised of like 17 data frames stacked on top of each other. BUT, each model has its name, so we can find the data for each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.regressors.df <- data.frame()\n",
    "\n",
    "for (this.regressor in colnames(X_train_lin)) {\n",
    "    \n",
    "    if (this.regressor %in% unique(motif.class$class)) {\n",
    "        glm.formula <- paste(\"ChIPseq.bound ~ \", \"as.factor(\", this.regressor, \")\",sep='')\n",
    "    } else {\n",
    "        glm.formula <- paste(\"ChIPseq.bound ~ \", this.regressor,sep='')\n",
    "    }\n",
    "\n",
    "    glm.df.train <- as.data.frame(cbind(y_train_lin, X_train_lin)) %>% rename(\"cs_hit\" = \"ChIPseq.bound\")\n",
    "    glm.df.test <-  as.data.frame(cbind(y_test_lin, X_test_lin)) %>% rename(\"cs_hit\" = \"ChIPseq.bound\")\n",
    "\n",
    "    glm.single <- glm(as.formula(glm.formula), data=glm.df.train, family=binomial)\n",
    "    glm.single$Model.Name <- paste(\"glm \", this.regressor, sep='')\n",
    "    \n",
    "    glm.pred.single.df <- make.pred.df.from.glm(glm.single, glm.df.test)\n",
    "    glm.stat.single.df <- make.stats.df.from.preds(glm.pred.single.df)\n",
    "    \n",
    "    stats.regressors.df <- rbind(stats.regressors.df, glm.stat.single.df)\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have all of our data frames stacked, we're going to filter out the 7 that are NOT a TF_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stats.regressors.df %>% \n",
    "filter(Model.Name %in% c(\"glm motifscore\",\n",
    "                         \"glm gc_content\",\n",
    "                         \"glm asinh_tss_dist\")) ->\n",
    "stats.regressors.filtered.df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## compare performance\n",
    "\n",
    "We're going to take the 7 single-factor linear models (stats.regressors.filtered), the gradient-boosted model (medium_stat), and the linear model with ALL regressors (glm.stat) and put them together. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all.stats.df <- rbind(\n",
    "    medium_stat_df,\n",
    "    glm.stat.df,\n",
    "    stats.regressors.filtered.df\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using another Rory function, we're going to plot the family of MCC curves. Note that the function:\n",
    "\n",
    "* Plots a line using `geom_line`, with X = sensitivity and Y = PPV (positive predictive value)\n",
    "* Separates the points by `Model.Name`\n",
    "* Makes the scales 0-1\n",
    "* Makes the plot square\n",
    "* Labels the axes\n",
    "\n",
    "Also note that the `theme_minimal` command controls the background (white) and whatnot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.mattcc.curve(all.stats.df) + theme_minimal(base_size = 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll do the same, only this time we'll plot the ROC curves. Recall that this is Sensitivity v. 1-Specificity, so Rory has named the axes accordingly. The function he wrote also allows you to label the area under the curve, but we don't do it here because....frankly, it'd be a mess with so many curves. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.roc.curve(all.stats.df) + theme_minimal(base_size = 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last plot is a bit trickier, we're plotting \"recall\" v. precision. What are these exactly?\n",
    "\n",
    "* Recall: sensitivity; how good you are at only picking out \n",
    "* Precision:  PPV; how well you can pick out only the true positives (P/TP)\n",
    "\n",
    "What we're actually plotting here is PPV (positive predictive value) versus sensitivity. So I think we're just using synonyms here. The rest of the function just scales the plot from 0-1.\n",
    "\n",
    "The idea is we should try to maximize both things, so we can see that the GBM model does the best, again\n",
    "\n",
    "** Is it possible that the axis labels are switched?? **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.precrecall.curve(all.stats.df) + theme_minimal(base_size = 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
